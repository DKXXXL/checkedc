\section{Appendix}\label{app:main}

\subsection{Meta Theories}\label{sec:theorem}

% Before we present our main theorems, we need to first
% discuss the meaning what a pointer being well-typed in a given heap
% snapshot $\heap$ means, which is captured by rules in
% Fig.~\ref{fig:const-type}. The variable type rule ($\textsc{T-Var}$)
% simply checks if a given variable has the defined type in $\Gamma$;
% the constant rule ($\textsc{T-Const}$) is slightly more involved.
% First, it ensures that the type annotation $\tau$ does not contain any
% free variables. More importantly, it ensures that the pointer points
% to a location that makes sense in a given heap.
%  
%  
%  The $\size$ function in Fig.~\ref{fig:const-type}
% refers to the \code{sizeof} function in C computing the number of
% bytes for a type.
%  
%  
%  Second, we
% require that any constant ($\evalue{n}{\tau}$) should make sense in
% $\heap$. We develop a recursive predicate $\sigma \vdash n : \tau$ to
% verify if $n$ has $\tau$ in a heap snapshot $\heap$. $\sigma$ is a
% constant set containing the constants that have been verified by the
% relation. For every constant $\evalue{n}{\tau}$, it is either an
% integer $\tint$, an unchecked pointer $\tptr{\omega}{\umode}$,
% zero-valued number ($n=0$), checked in $\sigma$
% ($\evalue{n}{\tptr{\omega}{\cmode}}\in \sigma$); or if it is not the
% above case, then (i) $\heap(n)$ is defined, and (ii) for every heap
% location $n+i$ in the range of the pointer (if $\omega$ is a word
% type, range is $[0,1)$; if $\omega$ is an array type
%   ($\tarray{0}{b_h}{\tau'}$), range is $[0,b_h)$, if $\tau$ is a
%     NT-array type ($\tntarray{0}{b_h}{\tau'}$), range is $[0,b_h+1)$),
%       if $\heap(n+i)=\evalue{n_a}{\tau_a}$, then
%       $\evalue{n_a}{\tau_a}$ satisfies $\sigma \cup \{(n,\tau) \}
%       \vdash n_a : \tau_a$.
%  
%  
% \begin{figure}[t]
% {\small
% \text{Type Rules for Constants and Variables:}
% \begin{mathpar}
%   \inferrule[T-Var]
%       {x : \tau \in \Gamma}
%       {\Gamma;\Theta \vdash_m x : \tau}
%  
%   \inferrule[T-Const]
%       {\fv(\tau) = \emptyset \\ \emptyset \vdash n : \tau}
%       {\Gamma;\Theta\vdash_m \evalue{n}{\tau} : \tau}
% \end{mathpar}
%     
% \text{Rules for Checking Constant Pointers In Heap:}
% \begin{mathpar}
%   \inferrule
%       {}
%       {\sigma \vdash n : \tint}
%  
%   \inferrule
%       {}
%       {\sigma \vdash n : \tptr{\omega}{\umode}}
%  
%   \inferrule
%       {}
%       {\sigma \vdash 0 : \tptr{\omega}{\cmode}}
%  
%   \inferrule
%       {\evalue{n}{\tptr{\omega}{\cmode}}\in \sigma}
%       {\sigma \vdash n : \tptr{\omega}{\cmode}}
%  
%   \inferrule
%       {\forall i \in [0,\size(\omega)) .
%            \sigma \cup \{(n:\tptr{\omega}{\cmode}) \}\vdash \heap(n+i)}
%       {\sigma \vdash n : \tptr{\omega}{\cmode}}
% \end{mathpar}
% }
% \caption{Type Rules for Checking Constants/Variables}
% \label{fig:const-type}
% \end{figure}

% \review{
%  Theorem 1 refers to a program $e$ being well-formed. Unless I've missed
%   something, I didn't see such a definition in the paper.}
% \mwh{This was stale text (dropped); $e$'s well formedness follows from the
%   assumption of well typing; we have added more details about that.}

In this subsection, we focus on our main meta-theoretic results about
\lang: type soundness (progress and preservation),
non-exposure, and non-crashing.
These proofs have been conducted in our Coq model.
Type soundness relies on several \emph{well-formedness} given in \ref{li22checkedc} and \Cref{appx:add-type-sem}.
Progress states that a \lang program can always make a move:

\begin{thm}[Progress]\label{thm:progress}

For any \lang program $e$, heap $\heap$, stack
$\varphi$, type environment $\Gamma$, and variable predicate set $\Theta$
that are all are well-formed, consistent
($\Gamma;\Theta\vdash \varphi$ and $\heap \vdash \varphi$) and well
typed ($\Gamma;\Theta\vdash_{\cmode} e : \tau$ for some $\tau$),
one of the following holds:

\begin{itemize}

\item $e$ is a value ($\evalue{n}{\tau}$).

\item there exists $\varphi'$ $\heap'$ $r$, such that $(\varphi,\heap,e) \longrightarrow_m (\varphi',\heap',r)$.

\end{itemize}
\end{thm}
%{\em Proof:} By induction on the typing derivation.

\noindent
There are two forms of preservation regarding the checked and unchecked regions.
Checked Preservation states that a reduction step preserves both the
type and consistency of the program being reduced.
Unchecked Preservation states that any evaluation happens at unchecked region does not affect the checked heap.

\begin{thm}[Checked Preservation]
For any \lang program $e$, heap $\heap$, stack
$\varphi$, type environment $\Gamma$, and variable predicate set $\Theta$
that are all are well-formed, consistent
($\Gamma;\Theta\vdash \varphi$ and $\heap \vdash \varphi$) and well
typed ($\Gamma;\Theta\vdash_{\cmode} e : \tau$ for some $\tau$), if there exists $\varphi'$,
$\heap'$ and $e'$, such that $(\varphi,\heap,e)
\longrightarrow_{\cmode} (\varphi',\heap',e')$, then $\heap'$ is
checked region consistent with $\heap$ ($\heap \triangleright \heap'$) and there exists
$\Gamma'$ and $\tau'$ that are well formed, checked region consistent
($\Gamma';\Theta\vdash \varphi'$ and $\heap' \vdash \varphi'$) and
well typed ($\Gamma';\Theta \vdash_{\cmode} e: \tau'$), where
$\tau'\sqsubseteq_{\Theta} \tau$.
\end{thm}
%{\em Proof:} By induction on the typing derivation.
%\smallskip
\begin{thm}[Unchecked Preservation]
For any \lang program $e$, heap $\heap$, stack
$\varphi$, type environment $\Gamma$, and variable predicate set $\Theta$
that are all are well-formed and well
typed ($\Gamma;\Theta\vdash_{\cmode} e : \tau$ for some $\tau$), if there exists $\varphi'$,
$\heap'$ and $e'$, such that $(\varphi,\heap,e)
\longrightarrow_{\umode} (\varphi',\heap',e')$, then $\heap'(\cmode)=\heap(\cmode)$.
\end{thm}

Using the above theorem, we first show the non-exposure theorem,
where code in unchecked region cannot observe a valid checked pointer address.

\begin{thm}[Non-Exposure]
For any \lang program $e$, heap $\heap$, stack
$\varphi$, type environment $\Gamma$, and variable predicate set $\Theta$
that are all are well-formed and well
typed ($\Gamma;\Theta\vdash_{\cmode} e : \tau$ for some $\tau$), if there exists $\varphi'$,
$\heap'$ and $e'$, such that $(\varphi,\heap,e)
\longrightarrow_{\umode} (\varphi',\heap',e')$ and $e=E[\alpha(x)]$ and $\mode(E)=\umode$,
where $\alpha(x)$ is some expression (not $\echeckedtext$ nor $\euncheckedtext$) containing variable $x$; 
thus, it is not a checked pointer.
\end{thm}

We now state our main result, {\em non-crashing},
which suggests that a well-typed program can never be \emph{stuck} (expression
$e$ is a non-value that cannot take a step\footnote{Note that
  $\ebounds$ and $\enull$ are \emph{not} stuck expressions---they represent a
  program terminated by a failed run-time check. A program that tries to access $\heap{n}$
  but $\heap$ is undefined at $n$ will be stuck, and violates spatial
  safety.}).

% \review{- There appears to be a slight discrepancy between the blame theorem in Coq and the one in the paper: the paper mentions some e', which I believe should be r. Also, the Coq code has a further disjunct m=Unchecked in the conclusion.}
% \liyi{It is a typo. We will add the thing back that we show that either user uses a unchecked mode to evaluate $e$ or $e$ lives in a context that is an unchecked region. This is a bit due to the space limitation. The semantic rules allow users to input the mode $m$ of evaluating an expression, I just forgot to include the $m$ in the result of the proof statement. }

\begin{thm}[Non-Crashing]\label{thm:blame} For any \lang
  program $e$, heap $\heap$, stack
$\varphi$, type environment $\Gamma$, and variable predicate set $\Theta$
that are well-formed and consistent
($\Gamma;\Theta\vdash \varphi$ and $\heap \vdash \varphi$),
if $e$ is well-typed ($\varphi;\Theta\vdash_{\cmode} e :
\tau$ for some $\tau$) and there exists
$\varphi_i$, $\heap_i$, $e_i$, and $m_i$ for $i\in [1,k]$, such that
$(\varphi,\heap,e) \longrightarrow_{m_1} (\varphi_1,\heap_1,e_1)\longrightarrow_{m_2} ...\longrightarrow_{m_k} (\varphi_k,\heap_k,r)$, then $r$ can never be \emph{stuck}.
\end{thm}

% \review{ 
% My interpretation of section IV.C is that the authors have a pen-and-paper
%   proof of Theorem 4, and that random testing using the Redex models has been
%   used to gain confidence in such a proof. Is this correct?}
% \liyi{Yes, this is correct.}
% \mwh{No, not correct: We have no pen-and-paper proof, just the
%   model. Clarify here}
% showing that
% extensive random testing fails to falsify the bisimilarity
% property. \mwh{Better way to state the previous?}

In the \systemname compiler, we also restore the compiler simulation theorem, such that executing a \systemname program is simulated by executing the compiled LLVM program. The details are given in the \checkedc formalism \cite{li22checkedc} and \Cref{appx:comp1}. 

We formalize both the compilation procedure and the simulation
theorem in the semantic model we developed in Redex (same as our Coq model),
and then attempt to falsify it via Redex's support for random
testing. Redex allows us
  to specify compilation as logical rules (an extension
  of typing), but then execute it algorithmically to
  automatically test whether simulation holds. This process revealed
  several bugs in compilation and the theorem statement.
%
  % us gain confidence that our original pen and paper proof of
  % simulation remains true with the addition of variable bounds. }
We ultimately plan to prove simulation in the Coq model.

%Turning to the simulation theorem: We first introduce notation
%used to specify the theorem.
We use the notation $\gg$ to
indicate the \emph{erasure} of stack and heap---the rhs is the same as
the lhs but with type annotations removed:
\begin{equation*}
  \begin{split}
    \heap  \gg & \dot \heap \\
    \varphi \gg & \dot \varphi
  \end{split}
\end{equation*}
In addition, when $\Gamma;\emptyset\vdash
\varphi$ and $\varphi$ is well-typed, we write $(\varphi,\heap,e) \gg_m (\dot \varphi, \dot \heap,
\dot e)$ to denote $\varphi \gg \dot \varphi$, $\heap \gg \dot \heap$
and $\Gamma;\Theta;\emptyset \vdash_m e \gg \dot e : \tau$ for some $\tau$ respectively. 
%$\Gamma$ is omitted from the notation since the well-formedness of $\varphi$ and its consistency with respect to $\Gamma$ imply that $e$ must be closed under $\varphi$, allowing us to recover $\Gamma$ from $\varphi$.
%Finally, we use $\xrightarrow{\cdot}^*$ to denote the transitive closure of the
%reduction relation of $\elang$. Unlike the $\lang$, the semantics of
%$\elang$ does not distinguish checked and unchecked regions.
We show an overview of the simulation theorem in \Cref{fig:checkedc-simulation-ref}.
The simulation theorem is specified in a way that is similar to the one by~\citet{merigoux2021catala}.

Previous ordinary simulation properties would
replace the middle and bottom parts of the figure with the
following: \[(\dot \varphi_0, \dot \heap_0, \dot e_0) 
  \xrightarrow{\cdot}^* (\dot \varphi_1, \dot \heap_1, \dot e_1)\]
Instead, we relate two erased configurations using the relation $\sim$,
which only requires that the two configurations will eventually reduce
to the same state.

% The two theorems are translation preservation and simulation. We donate $\xrightarrow{c}$ as the transition semantics of CLight.
\begin{thm}[Simulation ($\sim$)]\label{simulation-thm}
For \lang expressions $e_0$, stacks $\varphi_0$, $\varphi_1$, and heap snapshots $\heap_0$, $\heap_1$, 
if $\heap_0 \vdash \varphi_0$, $(\varphi_0,\heap_0,e_0)\gg_c (\dot \varphi_0,\dot \heap_0, \dot e_0)$,
and if there exists some $r_1$ such that $(\varphi_0, \heap_0, e_0)
\rightarrow_c (\varphi_1, \heap_1, r_1)$, then the following facts hold:

\begin{itemize}

\item if there exists $e_1$ such that $r=e_1$ and $(\varphi_1, \heap_1, e_1) \gg (\dot \varphi_1, \dot \heap_1, \dot e_1)$, then there exists some $\dot \varphi$,$\dot \heap$, $\dot e$, such that
$(\dot \varphi_0, \dot \heap_0,\dot e_0) \xrightarrow{\cdot}^* (\dot
\varphi,\dot \heap,\dot e)$ and $(\dot
\varphi_1,\dot \heap_1,\dot e_1) \xrightarrow{\cdot}^* (\dot \varphi,
\dot \heap,\dot e)$.

\item if $r_1 = \ebounds$ or $\enull$, then $(\dot \varphi_0, \dot \heap_0,\dot e_0) \xrightarrow{\cdot}^* (\dot
\dot \varphi_1,\dot \heap_1, r_1)$ where $\varphi_1 \gg \dot
\varphi_1$, $\heap_1 \gg \dot \heap_1$.

\end{itemize}
\end{thm}


% when $r_1 = e_1$ for
% some $e_1$ and
% $(\varphi_1, \heap_1, e_1) \gg (\dot \varphi_1, \dot \heap_1, \dot e_1)$, then
% there exists some $\dot \varphi$,$\dot \heap$, $\dot e$, such that
% $(\dot \varphi_0, \dot \heap_0,\dot e_0) \xrightarrow{\cdot}^* (\dot
% \varphi,\dot \heap,\dot e)$ and $(\dot
% \varphi_1,\dot \heap_1,\dot e_1) \xrightarrow{\cdot}^* (\dot \varphi,
% \dot \heap,\dot e)$. When $r_1 = \ebounds$ or $\enull$, we have $(\dot \varphi_0, \dot \heap_0,\dot e_0) \xrightarrow{\cdot}^* (\dot
% \dot \varphi_1,\dot \heap_1, r_1)$ where $\varphi_1 \gg \dot
% \varphi_1$, $\heap_1 \gg \dot \heap_1$.

% \begin{thm}[Simulation ($\sim$)]\label{simulation-thm}
% For \lang expressions $e_0$, stacks $\varphi_0$, $\varphi_1$, and heap snapshots $\heap_0$, $\heap_1$, 
% if $\emptyset;\emptyset;\emptyset \vdash_\cmode e_0 \gg \dot e_0 :\tau_0$,
% and if there exists some $r_1$ such that $(\varphi_0, \heap_0, e_0)
% \rightarrow_\cmode (\varphi_1, \heap_1, r_1)$, when $r_1 = e_1$ for
% some $e_1$ and
% $\emptyset;\emptyset;\emptyset \vdash_\cmode e_1 \gg \dot e_1 :\tau_1$ where $\tau_1 \sqsubseteq \tau_0$
% , then
% there exists some $\dot \varphi$,$\dot \heap$, $\dot e$, such that
% $(\dot \varphi_0, \dot \heap_0,\dot e_0) \xrightarrow{\cdot}^* (\dot
% \varphi,\dot \heap,\dot e)$ and $(\dot
% \varphi_1,\dot \heap_1,\dot e_1) \xrightarrow{\cdot}^* (\dot \varphi,
% \dot \heap,\dot e)$. When $r_1 = \ebounds$ or $\enull$, we have $(\dot \varphi_0, \dot \heap_0,\dot e_0) \xrightarrow{\cdot}^* (\dot
% \dot \varphi_1,\dot \heap_1, r_1)$ where $\varphi_1 \gg \dot
% \varphi_1$, $\heap_1 \gg \dot \heap_1$.
% \end{thm}

The current version of the Redex model has been tested against $27500$
test cases with depth less than $15$. Each expression can
reduce multiple steps, and we test simulation between every two
adjacent steps to cover a wider range of programs, particularly the
ones that have a non-empty heap.

\begin{figure}[t]
{\small
\[
\begin{array}{c}
\begin{tikzpicture}[
            > = stealth, % arrow head style
            shorten > = 1pt, % don't touch arrow head to node
            auto,
            node distance = 3cm
        ]

\begin{scope}[every node/.style={draw}]
    \node (A) at (0,1.5) {$\varphi_0,\heap_0, e_0$};
    \node (B) at (4,1.5) {$\varphi_1, \heap_1 ,e_1$};
    \node (C) at (0,0) {$\dot \varphi_0, \dot \heap_0 ,\dot e_0$};
    \node (D) at (4,0) {$\dot \varphi_1, \dot \heap_1, \dot e_1$};
    \node (E) at (2,-1.5) {$\dot \varphi,\dot \heap ,\dot e$};
\end{scope}
\begin{scope}[every edge/.style={draw=black}]

    \path [->] (A) edge node {$\longrightarrow_\cmode$} (B);
    \path [<->] (A) edge node {$\gg$} (C);
    \path [<->] (B) edge node {$\gg$} (D);
    \path [dashed,<->] (C) edge node {$\sim$} (D);
    \path [dashed,->] (C) edge node {$\xrightarrow{\cdot}^*$} (E);
    \path [dashed,->] (D) edge node[above] {$\xrightarrow{\cdot}^*$} (E);
\end{scope}

\end{tikzpicture}
\end{array}
\]
}
\caption{Simulation between \lang and \elang }
\label{fig:checkedc-simulation-ref}
\end{figure}

%{\em Proof:} By induction on the number of steps of the \checkedc
%evaluation ($\longrightarrow_m^*$), using progress and preservation to
%maintain the invariance of the assumptions.

% \review{
%   add a paragraph that discusses what are the main changes from [21] in terms
%   of the technical development (if there are any), e.g., are there any new
%   challenges that needed to be solved while proving the blame theorem for this
%   paper's semantics?
% }
%  Compared to \citet{ruef18checkedc-incr}, proofs for
%  \lang were made challenging by the addition of dependently typed
%  functions and dynamic arrays, and the need to handle bounds widening for NT
%  array pointers. These features required changes in the runtime
%%  semantics (adding a stack, and dynamically changing bounds) and in
 % compile-time knowledge of them (to soundly typing widened bounds).

% \ignore{
% \begin{figure}[t!]
%   \begin{prooftree}
%     \hypo{\evalue{n}{\tau} \in \defscope}
%     \infer1[T-VConst]{\Gamma;\defscope \vdash_m \evalue{n}{\tau}  : \tau}
%   \end{prooftree}
%   \qquad


%   \begin{prooftree}
%     \hypo{
%       \begin{matrix}
%         \Gamma;\defscope \vdash_m e : \tau \\
%         \Gamma;\defscope \vdash_m e_1 : \tau_1 
%       \end{matrix}
%     }
%     \hypo{
%       \begin{matrix}
% \Gamma \vdash_m \tau_3 = \tau_1 \vee \tau_2 \\
%             \Gamma;\defscope \vdash_m e_2 : \tau_2
%           \end{matrix}
%         }
%     \infer2[T-If]{\Gamma;\defscope \vdash_m \eif{e}{e_1}{e_2} : \tau_3}
%   \end{prooftree} \\ \\

%   \begin{prooftree}
%     \hypo{
%       \begin{matrix}
%         \Gamma;\defscope \vdash_m x : \tptr[c]{(\tntarray{l}{0}{\tau})}, l \leq 0 \\
%         \Gamma, x : \tptr[c]{(\tntarray{l}{1}{\tau})};\defscope \vdash_m e_1 : \tau_1 \\
%         \Gamma;\defscope \vdash_m e_2 : \tau_2 \\
%         \Gamma \vdash_m \tau_3 = \tau_1 \vee \tau_2
%       \end{matrix}
%     }
%     \infer1[T-IfNT]{\Gamma;\defscope \vdash_m \eif{\estar{x}}{e_1}{e_2} : \tau_3}
%   \end{prooftree} \\ \\

%   \begin{prooftree}
%     \hypo{
%       \begin{matrix}
%         F(f) = \tau_{j}\;(x_0:\tau_0, \ldots, x_{j-1}:\tau_{j-1})\;e  \\
%         \Gamma; \defscope \vdash_m \tau_i[x_0,\ldots,x_{i-1} \mapsto e_0,\ldots,e_{i-1}] ~~~ 0 \leq i \leq j\\
%         \Gamma; \defscope \vdash_m e_i : \tau_i' ~~~ 0 \leq i \leq j\\
%         \Gamma; \defscope \vdash_m \subtype{\tau_i'}{\tau_i[x_0,\ldots,x_{i-1} \mapsto e_0,\ldots,e_{i-1}]}  ~~~ 0 \leq i < j
%       \end{matrix}
%     }
%     \infer1[T-VCall]{\Gamma; \defscope \vdash_m f(\overline{e}) : \tau_j[x_0,\ldots,x_{j-1} \mapsto e_0,\ldots,e_{j-1}]}
%   \end{prooftree} \\ \\
% % \inferrule*[lab=T-PtrC]
% % {
% %   \tau = \tptr[c]{\omega} \\
% %   \tau_0, ..., \tau_{j-1} = \mathrm{types}(D,\omega)\\\\
% %   \wt[\Gamma][\defscope, n^\tau]{H(n+k)}{\tau_k} ~~~ 0 \leq k < j
% % %  \Gamma,n^\tau \proves H(n+k) : \tau_k~~~0 \leq k < j
% % }



%   \begin{prooftree}
%     \hypo{
%           \Gamma; \defscope \vdash_m e_1 : \tau_1
%       }
%       \hypo{        \Gamma, x = e_1 : \tau_1; \defscope \vdash_m e_2 : \tau_2
% }
%     \infer2[T-Let]{\Gamma; \defscope \vdash_m \elet{x}{e_1}{e_2} : \tau[x \mapsto e_1]}
%   \end{prooftree} \\ \\


%   \begin{prooftree}
%     \hypo{
%       \begin{matrix}
%         \Gamma; \defscope \vdash_m \estrlen{y} : \tau_1\\
%         \Gamma; \defscope \vdash_m y : \tptr[c]{(\tntarray{le}{\_}{\tau_3})}\\
%         \Gamma, x = \estrlen{y} : \tau_1, \\ y : \tptr[c]{(\tntarray{le}{x}{\tau_3})} ;  \defscope  \vdash_m e_2 : \tau_2
%         \end{matrix}
%         }
%         \infer1[T-LetStr]{\Gamma; \defscope \vdash_m \elet{x}{\estrlen{y}}{e_2} : \tau[x \mapsto e_1]}
%   \end{prooftree} \\ \\

%   \begin{prooftree}
%     \hypo{
%         \Gamma; \defscope \vdash_m e : \tptr[c]{(\tntarray{le}{he}{\tau})}
%       }
%         \infer1[T-Str]{\Gamma; \defscope \vdash_m \estrlen{y} : \tint}
%       \end{prooftree} \\ \\

%       \begin{prooftree}
%         \hypo{
%           \begin{matrix}
%             \tau = \tint \vee \tau = \tptr[u]{\omega}~\vee  n=0~ \vee \\ \tau = \tptr[c]{(\tarray{0}{0}{\tau'})} \vee \\
%             \tau = \tptr[c]{(\tntarray{0}{0}{\tau'})}
%         \end{matrix}
%       }
%       \infer1[T-Base]{\Gamma; \defscope \vdash_m \evalue{n}{\tau}  : \tau}
%     \end{prooftree} \\ \\

%     \begin{prooftree}
%       \hypo{
%         \begin{matrix}
%         \tau = \tptr[c]{\omega} \\
%         \tau_0, ..., \tau_{j-1} = \mathrm{types}(D,\omega)\\
%         \Gamma;\defscope, \evalue{n}{\tau}  \vdash_m {H(n+k)} : {\tau_k} ~~~ 0 \leq k < j
%       \end{matrix}
%       }
%       \infer1[T-PtrC]{\Gamma; \defscope \vdash_m \evalue{n}{\tau}  : \tau}
%     \end{prooftree} \\ \\

%     \begin{prooftree}
%       \hypo{
%         \begin{matrix}
%             \Gamma; \defscope \vdash_m e :  {\tptr[m]{\tstruct{T}}} \\
%             % \Gamma \proves e : \tptr[m]{\tstruct{T}} \\\\
%             D(T) = ...; \tau_f~f; ...
%         \end{matrix}
%       }
%       \infer1[T-Amper]{\Gamma; \defscope \vdash_m \eamper{e}{f} : \tptr[m]{\tau_f}}
%     \end{prooftree} \\ \\


%     \todo[inline]{YL: how to express le - n as a metafunction?}
%     \begin{prooftree}
%       \hypo{
%         \begin{matrix}
%         \Gamma; \defscope \vdash_m e_1 : \tptr[m']{(\tgarray{\alpha}{le}{he}{\tau})}\\
%         \Gamma; \defscope \vdash_m \evalue{n}{\tau}  : \tint \\
%         le' = le - n, he' = he - n
%       \end{matrix}
%     }
%       \infer1[T-BinopInd]{\Gamma; \defscope \vdash_m (e_1 \plus \evalue{n}{\tau} ) : \tptr[m']{(\tgarray{\alpha}{le'}{he'}{\tau})}}
%     \end{prooftree}
% \caption{Typing}
% \label{fig:typing}
% \end{figure}

% \begin{figure}[t!]
%   \todo[inline]{no sizeof check because we don't know statically whether the allocation would be null}
%   \begin{prooftree}
%     \hypo{
%       \omega = \tgarray{\alpha}{le}{he}{\tau} \Rightarrow le = 0
%     }
%     \infer1[T-Malloc]{\Gamma; \defscope \vdash_m \emalloc{\omega} : \tptr[c]{\omega}}
%   \end{prooftree}\\\\

%   \begin{prooftree}
%     \hypo{
%       \Gamma; \defscope \vdash_u e : \tau
%     }
%     \infer1[T-Unchecked]{\Gamma; \defscope \vdash_m \eunchecked{e} : \tau}
%   \end{prooftree}\\\\

%   \todo[inline]{any constraints on m'?}
%   \begin{prooftree}
%     \hypo{\Gamma; \defscope \vdash_m e :  \tptr[m']{(\tgarray{\alpha}{le'}{he'}{\tau})} }
%     \infer1[T-DynCast]{\Gamma; \defscope \vdash_m \edyncast{\tptr[m']{(\tgarray{\alpha}{le}{he}{\tau})}}{e} : \tau}
%   \end{prooftree}\\\\
  
%   \begin{prooftree}
%     \hypo{\Gamma; \defscope \vdash_m e : \tau'}
%     \infer1[T-Cast]{\Gamma; \defscope \vdash_m \ecast{\tau}{e} : \tau}
%   \end{prooftree}\\\\



%   \begin{prooftree}
%     \hypo{
%       \begin{matrix}
%         \Gamma; \defscope \vdash_m e_1 : \tptr[m']{\tgarray{\alpha}{le}{he}{\tau}} \\
%         \Gamma; \defscope \vdash_m e_2 : \tint \\
%         m' = u \Rightarrow m = u \\
%       \end{matrix}
%     }
%     \infer1[T-Index]{\Gamma; \defscope \vdash_m \estar{(\ebinop{e_1}{e_2})} : \tau}
%   \end{prooftree} \\\\


%   \begin{prooftree}
%     \hypo{
%       \begin{matrix}
%         \Gamma; \defscope \vdash_m e_1 : \tptr[m']{\omega} \\
%         \Gamma; \defscope \vdash_m e_2 : \tau \\
%         m' = u \Rightarrow m = u \\
%         \omega = \tau \vee \omega = \tgarray{\alpha}{le}{he}{\tau}
%       \end{matrix}
%     }
%     \infer1[T-Assign]{\Gamma; \defscope \vdash_m \eassign{e_1}{e_2} : \tau}
%   \end{prooftree}\\\\


%   \begin{prooftree}
%     \hypo{
%       \begin{matrix}
%         \Gamma; \defscope \vdash_m e_1 : \tptr[m']{\omega} \\
%         \Gamma; \defscope \vdash_m e_2 : \tint \\
%         \Gamma; \defscope \vdash_m e_3 : \tau \\
%         m' = u \Rightarrow m = u \\
%         \omega = \tau \vee \omega = \tgarray{\alpha}{le}{he}{\tau}
%       \end{matrix}
%     }
%     \infer1[T-IndAssign]{\Gamma; \defscope \vdash_m \eassign{(e_1 \plus e_2)}{e_3} : \tau}
%   \end{prooftree}

% \caption{Typing Cont.}
% \label{fig:typing2}
% \end{figure}
% \ignore{





% \begin{figure}[t!]
%   \begin{prooftree}
%     \hypo{
%       \begin{matrix}
%         \Gamma \vdash x_0:\tau_0, \ldots, x_{j}:\tau_{j} \\
%         \Gamma, x_0 = \kw{none} : \tau_0, \ldots, x_{j-1} = \kw{none} : \tau_{j-1}; \defscope \vdash e : \tau_{r}
%       \end{matrix}
%     }
%     \infer1[WF-Fun]{\Gamma;\defscope \vdash \tau_{j}\;(x_0:\tau_0, \ldots, x_{j-1}:\tau_{j-1})\;e}
%   \end{prooftree} \\ \\

%   \begin{prooftree}
%     \infer0[WF-Nil]{\Gamma \vdash \circ}
%   \end{prooftree} \\\\

%   \begin{prooftree}
%     \hypo{
%       \begin{matrix}
%         \Gamma \vdash \tau_0 \\
%         \Gamma, x_0 = \kw{none} :\tau_0 \vdash x_1:\tau_{1}, \ldots, x_{j}:\tau_{j}
%       \end{matrix}
%     }
%     \infer1[WF-Cons]{\Gamma \vdash x_0:\tau_0, \ldots, x_{j}:\tau_{j}}
%   \end{prooftree} \\\\

%   \begin{prooftree}
%     \infer0[WF-Int]{\Gamma \vdash \tint}
%   \end{prooftree} \\\\

%   \begin{prooftree}
%     \hypo{   \Gamma \vdash le }
%      \hypo{   \Gamma \vdash he }
%       \hypo{  \Gamma \vdash \tau}

%     \infer3[WF-Array]{\Gamma \vdash \tptr[m]{(\tgarray{\alpha}{le}{he}{\tau})}}
%   \end{prooftree} \\\\

%   \begin{prooftree}
%     \hypo{T \in dom(D)}
%     \infer1[WF-Struct]{\Gamma \vdash \tptr[m]{\tstruct{T}}}
%   \end{prooftree} \\\\

%   \begin{prooftree}
%     \hypo{\Gamma \vdash \tau}
%     \infer1[WF-Ptr]{\Gamma \vdash \tptr[m]{\tau}}
%   \end{prooftree} \\\\
  
%   \begin{prooftree}
%     \infer0[WFB-Int]{\Gamma \vdash i}
%   \end{prooftree}\\\\
  
%   \begin{prooftree}
%     \hypo{x = e? : \tint \in \Gamma}
%     \infer1[WFB-Var]{\Gamma \vdash x \plus i}
%   \end{prooftree} \\\\

%   \caption{Well-Formedness}
%   \label{fig:wf}
% \end{figure}



% \begin{figure}[t!]
%   \begin{prooftree}
%     \infer0[Sub-Nt]{\subtype{\tptr[c]{(\tntarray{le}{he}{\tau})}}{\tptr[c]{(\tarray{le}{he}{\tau})}}}
%   \end{prooftree} \\ \\

%   \begin{prooftree}
%     \infer0[Sub-Refl]{\subtype{\tau}{\tau}}
%   \end{prooftree} \\ \\

%   \todo[inline]{what about ntarrays?}
%   \begin{prooftree}
%     \infer0[Sub-Ptr]{\subtype{\tptr[c]{\tau}}{\tptr{\tarray{0}{1}{\tau}}}}
%   \end{prooftree} \\ \\

%   \todo[inline]{the subtyping relation is not anti-symmetric..}
%   \begin{prooftree}
%     \infer0[Sub-Arr]{\subtype{\tptr[c]{\tarray{0}{1}{\tau}}}{\tptr{\tau}}}
%   \end{prooftree} \\ \\

%   \begin{prooftree}
%     \hypo{\rboundle{le_0}{le_1}}
%     \hypo{\rboundle{he_1}{he_0}}
%     \infer2[Sub-Subsume]{\subtype{\tptr{(\tgarray{\alpha}{le}{he}{\tau})}{\cmode}}{ \tptr{(\tgarray{\alpha}{ le_1} {he_1} {\tau})}}}
%   \end{prooftree} \\ \\

%   \begin{prooftree}
%     % check t-amper below
%     \hypo{D(T) = \tau_f~f; ...}
%     \hypo{\subtype{\tau_f}{\tau}}
%     \infer2{\subtype{\tptr[c]{\tstruct{T}}}{\tau}}
%   \end{prooftree}
%   \caption{Subtyping}
%   \label{fig:sub}
% \end{figure}
% }
% }

% \ignore{

% \begin{figure*}[t]
%   \begin{lstlisting}
% int foo(nt_array_ptr<char> p : count(0)) {
%   if (* p) {
%     dyn_bounds_cast<nt_array_ptr<char>>(p, count(1));
%   }
%   dyn_bounds_cast<nt_array_ptr<char>>(p, count(1));
%   return 0;
% }
%   \end{lstlisting}
% \caption{The example where clang Checked C fails at run-time}
% \label{fig:clangbad1}
% \end{figure*}

% \begin{figure*}[t]
%   \begin{lstlisting}
% /* nt_array_ptr<char> p : bounds(p,p) */
% size_t cnt = 0;

% while(*(p+cnt)) {
%   ++cnt;
% }
% dyn_bounds_cast<nt_array_ptr<char>>(p, count(cnt));
%   \end{lstlisting}
% \caption{A useful program that the Checked C spec doesn't allow at run-time}
% \label{fig:clangbad2}
% \end{figure*}
% }

% \ignore{
% The Clang CheckedC implementation uses statically determined
% bounds to insert run-time checks. In Fig.~\ref{fig:clangbad1}, the
% \code{dyn_bounds_cast} at line 3 will always succeed, because the
% compiler knows that within the scope of then branch, the pointer
% \code{p} must have at least one element. The same cast at line 5,
% however, will always fail, since there is no way to tell statically
% whether the program has entered the then branch before. The compiler
% will check whether the \code{count(1)} bounds specification is
% contained within the earlier \code{count(0)} specification, resulting
% in a run-time failure even when we pass in a non-empty string.

% Our formalization diverges from this run-time behavior and instead keeps
% track of the bounds on the stack. After entering the then branch, we
% increment the upper bound for \code{p}, effectively making the
% updated bounds information available even after we exit the if
% statement. The cast at line 5 will be checking the new bounds against
% the incremented bounds for non-empty strings.

% Fig.~\ref{fig:clangbad2} gives a more practical example of why keeping
% track of the bounds on the stack is useful. The program snippet
% implements the functionality of the \code{strlen} function using a
% while loop and a \code{cnt} variable. Even though the type system is
% unable to reason about the while loop, as long as the runtime system
% updates the bounds in-place, the user can apply a
% \code{dyn_bounds_cast} to soundly recover the more precise bounds
% information.
% }


% \inferrule*[lab=T-Amper]
% {
%   \Gamma \proves e : \tptr[m]{\tstruct{T}} \\\\
%   D(T) = ...; \tau_f~f; ...
% }
% {
%   \Gamma \proves \eamper{e}{f} : \tptr[m]{\tau_f}
% }

\subsection{Differences with the Coq and Redex Models}\label{app:model-diffs}

% \begin{figure}[h]
% {\small
% {\captionsetup[lstlisting]{margin = 8 mm}
%   \begin{lstlisting}[xleftmargin=8 mm]
% Inductive expression : Type :=
%   | ELit : Z -> type -> expression
%   | EVar : var -> expression
%   | EStrlen : var -> expression
%   | ECall : funid -> list expression -> expression
%   | ERet : var -> Z* type -> expression -> expression
%   | EDynCast : type -> expression -> expression
%   | ELet : var -> expression -> expression -> expression
%   | EMalloc : type -> expression
%   | ECast : type -> expression -> expression
%   | EPlus : expression -> expression -> expression
%   | EFieldAddr : expression -> field -> expression
%   | EDeref : expression -> expression (*  * e *)
%   | EAssign : expression -> expression -> expression (* *e = e *)
%   | EIfDef : var -> expression -> expression -> expression (* if *x then e1 else e2. *)
%   | EIf : expression -> expression -> expression -> expression (* if e1 then e2 else e3. *)
%   | EUnchecked : expression -> expression.
%   \end{lstlisting}
% }
% }
% \caption{Expression Syntax in Coq}
% \label{fig:coq-syntax}
% \end{figure}

The Coq and Redex models of \lang may be found at \url{https://github.com/plum-umd/checkedc}.
The Coq model's syntax is slightly different from that in
Fig.~\ref{fig:checkc-syn}. In particular, the arguments in a function call
are restricted to variables and constants, according to
a separate well-formedness condition. A function call \code{f(e)} can
always be written in \code{let x = e in f(x)} to cope.
% In the Coq model, we do not restrict the function argument syntax. Instead, we achieve the restriction
% by restricting the type rule for each function argument element to only deal with constants and variables. 
% To avoid recursive inductive relations and simplify proofs, we define different inductive relations for typing function argument lists and elements. In the inductive relation of function argument elements, we only define rules for typing constants and variables, and the rules are copied from the main inductive type rules for constants and variables. 
%MWH: The above is nonsensical. The answer is: We have an expresion
%well-formedness condition that restricts this syntax, among other things.
In addition,
conditionals have two syntactic forms: \code{EIf} is a normal
conditional, and \code{EIfDef} is one whose boolean guard is of the
form \estar{x}. By syntactically distinguishing these two
cases, the Coq model does not need the \emph{[prefer]} rule for
\code{if (*x)}$...$ forms as in
Fig.~\ref{fig:c-context}. The Redex model \emph{does} prioritize such
forms but not the same way as in the figure. It uses a variation of
the \textsc{S-Var} rule: The 
modified rule is equipped with a precondition that is false whenever
\textsc{S-IfNTT} is applicable.
% \mwh{Do we need
%   to talk about other semantic judgments that are not in the paper,
%   e.g., well-formedness? Maybe not, but should make sure the code is
%   commented to clarify that these things are not strictly needed --
%   well-formedness is subsumed by well typing (if that's true)?} \liyi{I will add comments to make sure it is clear in Coq.}
% \yiyun{No, well-formedness is not subsumed by the typing relation in
%   the Coq model. I
%   checked the latest coq code; they must be explicitly stated in the theorems.}
% \mwh{I mean: We might have it in the coq model, but I suspect it is
%   not necessary. If it is, we should be saying something in the main
%   paper. 

The Coq model uses a runtime stack $\varphi$ as described at the start
of Sec.~\ref{sec:semantics}.
The Redex model introduces let bindings during evaluation to
simulate a runtime stack. For example, consider the expression 
$e \equiv \elet{x}{(\evalue{5}{\tint})}{\ebinop{x}{x}}$. Expression $e$ first steps to
$\elet{x}{(\evalue{5}{\tint})}{\ebinop{(\evalue{5}{\tint})}{x}}$, which in
turns steps to
$\elet{x}{(\evalue{5}{\tint})}{\ebinop{(\evalue{5}{\tint})}{(\evalue{5}{\tint})}}$. Since
the rhs of $x$ is a value, the let binding in $e$ effectively
functions as a stack that maps from $x$ to $\evalue{5}{\tint}$. The
let form remains in the expression and lazily replaces the variables
in its body. The let form can be removed from the expression only if its
body is evaluated to a value, e.g., 
$\elet{x}{(\evalue{5}{\tint})}{(\evalue{10}{\tint})}$
steps to 
$\evalue{10}{\tint}$. The rule for popping let bindings in this manner
corresponds to the \textsc{S-Ret} rule in Fig.~\ref{fig:semantics}.
Leveraging let bindings adds complexity to the semantics but
simplifies typing/consistency and term generation during randomized testing.
%  \mwh{I don't understand the last sentence. Nowhere in the
%   paper do we talk about ANF. Also: Isn't it the case that there are
%   rules for let bindings do not use substitution, but rather ``look
%   up'' the binding on the definition chain? E.g.,
%   \code{let x=5 in x+x} does not step to \code{5+5} but instead steps
%   to \code{let x=5 in 5+5} and then to \code{let x=5 in 10} ? }
% \liyi{Yiyun: please see here. If you do not know, please explain to me and then we can make a correct statement. }
% \yiyun{It is supposed to refer to the ANF transformation of the erased language, not the source
%   language. I moved it to the discussion of ANF form below in \ref{appx:comp1}. }

\subsection{Typing Rules for Literal Pointers}\label{sec:literal-pointer-typing}

The typing of integer literals, which can also be pointers to the
heap, was presented in Sec.~\ref{sec:theorem} in Fig.~\ref{fig:const-type}. Here
we describe these rules further.

The variable type rule
($\textsc{T-Var}$) simply checks if a given variable has the defined
type in $\Gamma$; the constant rule ($\textsc{T-Const}$) is slightly
more involved.  First, it ensures that the type annotation $\tau$ does
not contain any free variables. More importantly, it ensures that the
literal itself is well typed using an auxilliary typing relation
$\heap;\sigma \vdash n : \tau$.

If the literal's type is an integer, an unchecked pointer, or a null
pointer, it is well typed, as shown by the top three rules in
Fig.~\ref{fig:const-type}. However, if it is a checked pointer
$\tptr{\omega}{\cmode}$, we need to ensure that what it points to in
the heap is of the appropriate pointed-to type ($\omega$), and also
recursively ensure that any literal pointers reachable this way are
also well-typed. This is captured by the bottom rule in the figure,
which states that for every location $n+i$ in the pointers' range
%
$[n, n+\size(\omega))$, where $\size$ yields the size of its argument,
  then the value at the location $\heap(n+i)$ is also well-typed.
  However, as heap snapshots can contain cyclic structures (which
  would lead to infinite typing deriviations), we use a scope $\sigma$
  to assume that the original pointer is well-typed when checking the
  types of what it points to. The middle rule then accesses the scope
  to tie the knot and keep the derivation finite, just like in
  \citet{ruef18checkedc-incr}.

\subsection{Other Semantic Rules}\label{sec:rem-semantics}

Fig.~\ref{fig:rem-semantics} shows the remaining semantic rules for
$\lang$. We explain a selected few rules in this subsection.
% other few low-level semantic rules for variable and dereference and $\emalloctext$ operations in \checkedc. Other operations are defined in the same manner.

\begin{figure*}[t]
{\small
\begin{mathpar}

\inferrule [S-Var]{} {(\varphi,\heap,x)\longrightarrow (\varphi,\heap,\varphi(x))}

    \inferrule[S-DefArray]{\heap(n)=\evalue{n_a}{\tau_a} \\ 0 \in [n_l,n_h)}
    {(\varphi,\heap,\estar{\evalue{n}{\tntarrayptr{n_l}{n_h}{\tau}{\cmode}}}) \longrightarrow (\varphi,\heap,\evalue{n_a}{\tau})}


    \inferrule[S-DefArrayBound]{0 \not\in [n_l,n_h)}
     { (\varphi,\heap,\estar{\evalue{n}{\tallarrayptr{n_l}{n_h}{\tau}{c}}}) \longrightarrow (\varphi,\heap,\ebounds)}

    \inferrule[S-DefNTArrayBound]{0 \notin [n_l,n_h]}
    {(\varphi,\heap,\estar{\evalue{n}{\tntarrayptr{n_l}{n_h}{\tau}{\cmode}}}) \longrightarrow (\varphi,\heap,\ebounds)}


    \inferrule[S-Assign]{\heap(n)=\evalue{n_a}{\tau_a} }
      {(\varphi,\heap,\eassign{\evalue{n}{\tptr{\tau}{\cmode}}}{\evalue{n_1}{\tau_1}}) \longrightarrow (\varphi,\heap[n \mapsto \evalue{n_1}{\tau}],\evalue{n_1}{\tau})}

    \inferrule[S-AssignNull]{}
      {(\varphi,\heap,\eassign{\evalue{0}{\tptr{\omega}{\cmode}}}{\evalue{n_1}{\tau_1}}) \longrightarrow (\varphi,\heap,\enull)}

    \inferrule[S-AssignArrBound]{0 \not\in [n_l,n_h) }
      {(\varphi,\heap,\eassign{\evalue{n}{\tallarrayptr{n_l}{n_h}{\tau}{\cmode}}}{\evalue{n_1}{\tau_1}}) \longrightarrow (\varphi,\heap,\ebounds)}

  \inferrule[S-Malloc]{\varphi(\omega)=\omega_a \\ \mathtt{alloc}(\heap,\omega_a)=(n,\heap')}
   { (\varphi,\heap,\emalloc{\omega}) \longrightarrow (\varphi,\heap',\evalue{n}{\tptr{\omega_a}{\cmode}})}

  \inferrule[S-MallocBound]{\varphi(\omega)=\tallarray{n_l}{n_h}{\tau}\\ (n_l \neq 0 \vee n_h \le 0)}
    { (\varphi,\heap,\emalloc{\omega}) \longrightarrow (\varphi,\heap',\ebounds)}

    \inferrule[S-IfT]{n \neq 0 }
    {(\varphi,\heap,\eif{\evalue{n}{\tau}}{e_1}{e_2}) \longrightarrow (\varphi,\heap,e_1)}

    \inferrule[S-IfF]{}
    {(\varphi,\heap,\eif{\evalue{0}{\tau}}{e_1}{e_2}) \longrightarrow (\varphi,\heap,e_2)}

    \inferrule[S-Unchecked]{}
    {(\varphi,\heap,\eunchecked{\evalue{n}{\tau}} \longrightarrow (\varphi,\heap,\evalue{n}{\tau})}

    \inferrule[S-Str]{
        0 \in [n_l,n_h]
\\ n_a \le n_h
\\ \heap(n+n_a) = 0 
\\ (\forall i. n \le i < n+n_a \Rightarrow (\exists n_i\;t_i. \heap(n+i) = \evalue{n_i}{\tau_i} \wedge n_i \neq 0))}
    {(\varphi,\heap,\estrlen{\evalue{n}{\tarrayptr{n_l}{n_h}{\tau}{m}}}) \longrightarrow (\varphi,\heap,\evalue{n_a}{\tint})}

    \inferrule[S-StrBounds]{
        0 \notin [n_l,n_h]
}
    {(\varphi,\heap,\estrlen{\evalue{n}{\tarrayptr{n_l}{n_h}{\tau}{c}}}) \longrightarrow (\varphi,\heap,\ebounds)}

    \inferrule[S-StrNull]{}
    {(\varphi,\heap,\estrlen{\evalue{0}{\tarrayptr{n_l}{n_h}{\tau}{c}}}) \longrightarrow (\varphi,\heap,\enull)}

    \inferrule[S-Add]{n = n_1 + n_2}
    {(\varphi,\heap,\evalue{n_1}{\tint} \plus \evalue{n_2}{\tint}) \longrightarrow (\varphi,\heap, n)}

    \inferrule[S-AddArr]{n = n_1 + n_2\\ n_l' = n_l - n_2 \\ n_h' = n_h - n_2}
    {(\varphi,\heap,\evalue{n_1}{\tallarrayptr{n_l}{n_h}{\tau}{m}} \plus \evalue{n_2}{\tint}) \longrightarrow (\varphi,\heap, \evalue{n}{\tallarrayptr{n_l'}{n_h'}{\tau}{m}})}

n    \inferrule[S-AddArrNull]{}
    {(\varphi,\heap,\evalue{0}{\tallarrayptr{n_l}{n_h}{\tau}{c}} \plus \evalue{n_2}{\tint}) \longrightarrow (\varphi,\heap, \enull)}

\end{mathpar}

}
\caption{Remaining \lang Semantics Rules (extends Fig.~\ref{fig:semantics})}
\label{fig:rem-semantics}
\end{figure*}

Rule \textsc{S-Var} loads the value for $x$ in stack $\varphi$.
Rule \textsc{S-DefArray} dereferences an array pointer, which is similar to the Rule \textsc{S-DefNTArray} in Fig.~\ref{fig:semantics} (dealing with null-terminated array pointers).
The only difference is that the range of $0$ is at $[n_l,n_h)$ not $[n_l,n_h]$, meaning that one cannot dereference the upper-bound position in an array.
Rules \textsc{DefArrayBound} and \textsc{DefNTArrayBound} describe an error case for a dereference operation.
If we are dereferencing an array/NT-array pointer and the mode is $\cmode$, $0$ must be in the range from $n_l$ to $n_h$ (meaning that the dereference is in-bound); if not, the system results in a $\ebounds$ error. Obviously, the dereference of an array/NT-array pointer also experiences a $\enull$ state transition if $n\le 0$.

Rules \textsc{S-Malloc} and \textsc{S-MallocBound} describe the $\emalloctext$ semantics. Given a valid type $\omega_a$ that contains no free variables, $\mathtt{alloc}$ function returns an address pointing at the first position of an allocated space whose size is equal to the size of $\omega_a$, and a new heap snapshot $\heap'$ that marks the allocated space for the new allocation. The $\emalloctext$ is transitioned to the address $n$ with the type ${\tptr{\omega_a}{\cmode}}$ and new updated heap. It is possible for $\emalloctext$ to transition to a $\ebounds$ error if the $\omega_a$ is an array/NT-array type $\tallarray{n_l}{n_h}{\tau}$, and either $n_l \neq 0$ or $n_h \le 0$. This can happen when the bound variable is evaluated to a bound constant that is not desired. 


\subsection{Subtyping for dependent types}
\label{app:le}
  
The subtyping relation given in Fig.~\ref{fig:checkc-subtype} involves
dependent bounds, i.e., bounds that may refer to variables. To decide
premises $b \leq b'$, we need a decision procedure that accounts for
the possible values of these variables. This process considers
$\Theta$, tracked by the typing judgment, and $\varphi$, the current
stack snapshot (when performing subtyping as part of the type
preservation proof).

\begin{defi}[Inequality]

\begin{itemize}

\item $n \le m$ if $n$ is less than or equal to $m$.
\item $x+n \le x + m$ if $n$ is less than or equal to $m$.
\item All other cases result in $\efalse$.

\end{itemize}
\end{defi}

To capture bound variables in dependent types, the \checkedc subtyping
relation ($\sqsubseteq$) is parameterized by a restricted stack
snapshot $\varphi|_{\rho}$ and the predicate map $\Theta$, where
$\varphi$ is a stack and $\rho$ is a set of
variables. $\varphi|_{\rho}$ means to restrict the domain of $\varphi$
to the variable set $\rho$. Clearly, we have the relation:
$\varphi|_{\rho} \subseteq \varphi$. $\sqsubseteq$
being parameterized by $\varphi|_{\rho}$ refers to that when we
compare two bounds $b \le b'$, we actually do
$\varphi|_{\rho}(b) \le \varphi|_{\rho}(b')$ by interpreting the
variables in $b$ and $b'$ with possible values in $\varphi|_{\rho}$.
Let's define a subset relation $\preceq$ for two restricted stack
snapshot $\varphi|_{\rho}$ and $\varphi'|_{\rho}$:

\begin{defi}[Subset of Stack Snapshots]
  Given two $\varphi|_{\rho}$ and $\varphi'|_{\rho}$,
  $\varphi|_{\rho} \preceq \varphi'|_{\rho}$, iff for $x\in\rho$ and
  $y$,
  $(x,y) \in \varphi|_{\rho} \Rightarrow (x,y) \in \varphi'|_{\rho}$.
\end{defi}

For every two restricted stack snapshots $\varphi|_{\rho}$ and
$\varphi'|_{\rho}$, such that
$\varphi|_{\rho} \preceq \varphi'|_{\rho}$, we have the following
theorem in \checkedc (proved in Coq):

\begin{thm}[Stack Snapshot Theorem]
  Given two types $\tau$ and $\tau'$, two restricted stack snapshots
  $\varphi|_{\rho}$ and $\varphi'|_{\rho}$, if
  $\varphi|_{\rho}\preceq \varphi'|_{\rho}$, and
  $\tau \sqsubseteq \tau'$ under the parameterization of
  $\varphi|_{\rho}$, then $\tau \sqsubseteq \tau'$ under the
  parameterization of $\varphi'|_{\rho}$.
\end{thm}

Clearly, for every $\varphi|_{\rho}$, we have
$\emptyset \preceq \varphi|_{\rho}$. The type checking stage is a
compile-time process, so $\varphi|_{\rho}$
is $\emptyset$ at the type checking stage. Stack snapshots are needed
for proving type preserving, as variables in bounds expressions are
evaluated away.


\begin{figure}[h!]
{\small
  \begin{mathpar}
    \inferrule[T-Def]
              {\Gamma;\Theta \vdash_m e : \tptr{\tau}{m'} \\
                m \leq m'}
              {\Gamma;\Theta \vdash_m \estar{e} : \tau}

    \inferrule[T-Mac]
              {}
              {\Gamma; \Theta \vdash_m \emalloc{\omega} : \tptr{\omega}{\cmode}}

    \inferrule[T-Add]
              {\Gamma; \Theta \vdash_m e_1 : \tint \\
                \Gamma; \Theta \vdash_m e_2 : \tint}
              {\Gamma; \Theta \vdash_m (e_1 \plus e_2) : \tint }

    \inferrule[T-Ind] 
              {\Gamma; \Theta \vdash_m e_1 : \tptr{\tallarrayb{\bvar}{\tau}}{m'} \\
                \Gamma; \Theta \vdash_m e_2 : \tint \\
                m \leq m'}              
              {\Gamma; \Theta \vdash_m \estar{(\ebinop{e_1}{e_2})} : \tau}

    \inferrule[T-Assign]
              {\Gamma; \Theta \vdash_m e_1 : \tptr{\tau}{m'} \\
                \Gamma; \Theta \vdash_m e_2 : \tau' \\
                \tau'\sqsubseteq \tau \\
                m \leq m'}
              {\Gamma; \Theta \vdash_m \eassign{e_1}{e_2} : \tau}

   \inferrule[T-IndAssign]
              {\Gamma; \Theta \vdash_m e_1 : \tptr{\tallarrayb{\bvar}{\tau}}{m'}\\
                \Gamma; \Theta \vdash_m e_2 : \tint \\
                \Gamma; \Theta \vdash_m e_3 : \tau' \\
                \tau'\sqsubseteq \tau \\
                m \leq m'}
              {\Gamma; \defscope \vdash_m \eassign{(e_1 \plus e_2)}{e_3} : \tau}

  \end{mathpar}
}
\caption{Remaining \lang Type Rules (extends Fig.~\ref{fig:type-system-1})}
\label{fig:rem-type-system}
\end{figure}

As mentioned in the main text, $\sqsubseteq$ is also parameterized by
$\Theta$, which provides the range of allowed values for a bound
variable; thus, more $\sqsubseteq$ relation is provable. For example,
in Fig.~\ref{fig:strcat-ex}, the \code{strlen} operation in line 4
turns the type of \code{dst} to be $\tntarrayptr{0}{x}{\tint}{\cmode}$
and extends the upper bound to \code{x}. In the \code{strlen} type
rule, it also inserts a predicate \code{x}$\ge 0$ in $\Theta$; thus,
the cast operation in line 16 is valid because
$\tntarrayptr{0}{x}{\tint}{\cmode} \sqsubseteq
\tntarrayptr{0}{0}{\tint}{\cmode}$ is provable when we know
\code{x}$\ge 0$.


Note that if $\varphi$ and $\Theta$ are $\emptyset$, we do only the
syntactic $\le$ comparison; otherwise, we apply $\varphi$ to both
sides of $\sqsubseteq$, and then determine the $\le$ comparasion based
on a Boolean predicate decision procedure on top of $\Theta$. This
process allows us to type check both an input expression and the
intermediate expression after evaluating an expression. 

\subsection{Other Type Rules}\label{rem-type}

Here we show the type rules for other \checkedc operations in Fig.~\ref{fig:rem-type-system}.
Rule \textsc{T-Def} is for dereferencing a non-array pointer. 
The statement $m \leq m'$ ensures that no unchecked pointers are used in checked regions.
Rule \textsc{T-Mac} deals with
$\emalloctext$ operations. There is a well-formedness check to require
that the possible bound variables in $\omega$ must be in the domain of
$\Gamma$ (see Fig.~\ref{fig:wftypesandbounds}). This is similar to the well-formedness assumption of the type environment (Definition~\ref{type-wellformed}) Rule \textsc{T-Add} deals with binary operations whose sub-terms are integer expressions, while rule \textsc{T-Ind} serves the case for pointer arithmetic. For simplicity, in the \checkedc formalization, we do not allow arbitrary pointer arithmetic. The only pointer arithmetic operations allowed are the forms shown in rules \textsc{T-Ind} and \textsc{T-IndAssign} in Fig.~\ref{fig:rem-type-system}. Rule \textsc{T-Assign} assigns a value to a non-array pointer location. The predicate $\tau'\sqsubseteq \tau$ requires that the value being assigned is a subtype of the pointer type.
The \textsc{T-IndAssign} rule is an extended assignment operation for handling assignments for array/NT-array pointers with pointer arithmetic. Rule \textsc{T-Unchecked} type checks \code{unchecked} blocks.

\subsection{Struct Pointers}\label{appx:struct}

\checkedc has \kw{struct} types and \kw{struct} pointers. Fig.~\ref{fig:checkc-struct} contains the syntax of \kw{struct} types as well as new subtyping relations built on the \kw{struct} values.
For a \kw{struct} typed value, \checkedc has a special operation for it, which is $\eamper{e}{f}$. This operation indexes the $f$-th position \kw{struct}~$T$ item, if the expression $e$ is evaluated to a \kw{struct} pointer $\tptr{\tstruct{T}}{m}$. Rule \textsc{T-Struct} in Fig.~\ref{fig:checkc-struct} describes its typing behavior.
Rules \textsc{S-StructChecked} and \textsc{S-StructUnChecked} describe the semantic behaviors of $\eamper{e}{f}$ on a given \kw{struct} \code{checked}/\code{unchecked} pointers, while rule \textsc{S-StructNull} describes a \code{checked} \kw{struct} null-pointer case.
In our Coq/Redex formalization, we include the \kw{struct} values and the operation $\eamper{e}{f}$. We omit it in the main text due to the paper length limitation.

\begin{figure}
{\small
$\begin{array}{l}
\text{  Struct Syntax: }\\[0.5em]
  \begin{array}{ll}
 \mathtt{Type} & \tstruct{T}
\\[0.2em]
     \text{Structdefs} & D \; \in \; T \rightharpoonup fs \\[0.2em]
      \text{Fields} & fs \; ::= \; \tau~\mathtt{f} \mid \tau~\mathtt{f}; fs 
    \end{array}\\[2em]
\text{  Struct Subtype: }\\[0.5em]
\begin{array}{l}
    D(T) = fs \wedge fs(0) = \tnat \Rightarrow  \tptr{\tstruct{T}}{m} \sqsubseteq \tptr{\tnat}{m}\\[0.5em]
    D(T) = fs \wedge fs(0) = \tnat \wedge 0 \le b_l \wedge b_h \le 1 \\[0.2em]
 \qquad\qquad \Rightarrow 
       \tptr{\tstruct{T}}{m} \sqsubseteq \tarrayptr{b_l}{b_h}{\tnat}{m}
    \end{array}
\\[3em]
\text{  Struct Type Rule: }\\[0.5em]
    \end{array}
$

{
\begin{mathpar}
  \inferrule [T-Struct]
  {\Gamma; \Theta \vdash_m e :  {\tptr{\tstruct{T}}{m}} \\
    D(T) = fs \\ fs(f)=\tau_f}
  {\Gamma; \Theta \vdash_m \eamper{e}{f} : \tptr{\tau_f}{m}}

\end{mathpar}
}
{
$\begin{array}{l}
\text{Struct Semantics: }
\end{array}
$
}
{
\begin{mathpar}
  \inferrule [S-StructChecked]
  {n > 0 \\ D(T) = fs \\ fs(f)=\tau_a \\ n_a=\mathtt{index}(fs,f)}
  {(\varphi,\heap,\eamper{\evalue{n}{\tptr{\tstruct{T}}{\cmode}}}{f}) \longrightarrow (\varphi,\heap,\evalue{n_a}{\tptr{\tau_a}{\cmode}})}

  \inferrule [S-StructNull]
  {n = 0}
  {(\varphi,\heap,\eamper{\evalue{n}{\tptr{\tstruct{T}}{\cmode}}}{f}) \longrightarrow (\varphi,\heap,\enull)}

  \inferrule [S-StructUnChecked]
  {D(T) = fs \\ fs(f)=\tau_a \\ n_a=\mathtt{index}(fs,f)}
  {(\varphi,\heap,\eamper{\evalue{n}{\tptr{\tstruct{T}}{\umode}}}{f}) \longrightarrow (\varphi,\heap,\evalue{n_a}{\tptr{\tau_a}{\umode}})}

\end{mathpar}
}
}
  \caption{\lang Struct Definitions}
  \label{fig:checkc-struct}
\end{figure}

\begin{figure}[t]
{\small
  \begin{mathpar}

    \inferrule[]
    {}
    {\Gamma \vdash n}

    \inferrule[]
    {x:\tint \in \Gamma}
    {\Gamma \vdash x + n}

    \inferrule[]
    {\Gamma \vdash b_l\\
    \Gamma \vdash b_h}
  {\Gamma \vdash (b_l,b_h)}

  \inferrule[]
  {}
  {\Gamma \vdash \tint}

  \inferrule[]
  {\Gamma \vdash \bvar \\
  \Gamma \vdash \tau}
  {\Gamma \vdash \tptr{\tallarrayb{\bvar}{\tau}}{m}}

  \inferrule[]
  {\Gamma \vdash \tau}
  {\Gamma \vdash \tptr{\tau}{m}}

  \inferrule[]
  {T \in D}
  {\Gamma \vdash \tptr{\tstruct{T}}{m}}
  \end{mathpar}
}
 \caption{Well-formedness for Types and Bounds}
\label{fig:wftypesandbounds}
\end{figure}


% \inferrule[T-Fun]
%     {\Xi(f) = \tau\;(\overline{x}:\overline{\tau})\;e \\\\
%         \Gamma; \Theta \vdash_m \overline{e} : \overline{\tau'} \\
%              \overline{\tau'} \sqsubseteq 
%                \overline{\tau}[\overline{e} / \overline{x}]}
%     {\Gamma; \Theta \vdash_m f(\overline{e}) : \tau[\overline{e} / \overline{x}]}


\begin{figure}[t]
{\small
  \begin{mathpar}
    \inferrule[]
    {\Gamma \vdash \overline{x}:\overline{\tau} \\
      \Gamma[\overline{x} \mapsto \overline{\tau}] \vdash \tau \\
    \Gamma[\overline{x} \mapsto \overline{\tau}]; \Theta  \vdash_{\cmode} e : \tau}
    {\Gamma \vdash \tau\;(\overline{x}:\overline{\tau})\;e}

    \inferrule[]
    {}
    {\Gamma \vdash \cdot}

    \inferrule[]
    {\Gamma \vdash \tau \\
    \Gamma[x \mapsto \tau] \vdash \overline{x}:\overline{\tau}}
    {\Gamma \vdash x:\tau, \overline{x}:\overline{\tau}}
  \end{mathpar}
}
 \caption{Well-formedness for functions}
\label{fig:wffunctions}
\end{figure}


\begin{figure}[t]
{\small
  \begin{mathpar}
    \inferrule[]
    {\Gamma \vdash \tau}
    {\Gamma \vdash \tau~\mathtt{f} }

    \inferrule[]
    {\Gamma \vdash \tau\\
     \Gamma \vdash fs}
    {\Gamma \vdash \tau~\mathtt{f}; fs }

  \end{mathpar}
}
 \caption{Well-formedness for structs}
\label{fig:wfstructs}

{\small
  \begin{mathpar}

    \inferrule[]
    {\Gamma[\overline{x} \mapsto \overline{\tau}]; \emptyset  \vdash
      e \gg \dot e : \tau}
    {\Gamma \vdash \tau\;(\overline{x}:\overline{\tau})\;e \gg
      (\overline{x})\;\dot e}
  \end{mathpar}
}
 \caption{Compilation Rules for Functions}
\label{fig:compilefunctions}
\end{figure}

\subsection{The Compilation Rules}\label{appx:comp1}

% \review{
% The first reviewer said:
% \begin{itemize}
% \item I had a really hard time understanding precisely the "formalized" compilation
%   from CoreChkC to CoreC. Specifically: is CoreC intended to model LLVM-IR, or
%   is it a subset of C? Is CheckedC compiling to C or is it a new frontend like
%   `clang`? See remark about undefined behavior (UB) below which got me even more
%   confused. \mwh{CoreC is meant to model CoreChkC but with
%   annotations, and checks removed; it is not meant to model C or LLVM-IR.}
% \item Is the compilation scheme from CoreChkC to CoreC is faithful to the actual
%   compilation scheme of the checkedc-clang compiler to ...? I feel like the
%   paper is missing an actual description of what happens in the compiler to
%   allow us to connect the dots and understand how the formalization illuminates
%   the implementation. \mwh{We did not model compilation on the real
%   implementation; our purpose was to show that annotations in CoreChkC
%   do not necessitate fat pointers in an implementation; that said,
%   our formalization does show how a real implementation can be
%   carried out}
% \item The paper doesn't seem upfront about what is *shown* (theorem in Coq) and what
%   is *tested* (via PLT-Redex), and thus remains a
%   conjecture. \mwh{Updated the intro and the individual sections}
% \item  Missing discussion of why Coq vs. PLT-Redex, effort involved, any plans to
%   formally prove compilation from CoreChkC to CoreC, any hopes of integrating
%   that in the official implementation, etc. \mwh{Added note at the
%   start of section 3.}
% \end{itemize} }

% \review{IV: ghost variables in other contexts (e.g. Why3, Dafny) are used for things
%   that do not exist at run-time, but this doesn't seem to be the case here.}
% \yiyun{Agreed: We changed the name to ``shadow variables'' to avoid confusion}
% \review{Do your bug report and github links break anonymity?}


% \review{From reviewer C:
% \begin{itemize}
% \item I'd like to have seen a bit more motivation for using PLT redex:
%   what aspects made the use of this tool preferable to formulating the
%   compilation in Coq and using Quickchick to do random testing of the
%   simulation result.
%   \mwh{Added some text to the end of III.A and IV.C}
% \yiyun{Some reasons I can think of:\begin{itemize}
%     \item Redex is highly optimized for specifying judgments that are algorithmic. By writing down a typing relation, we can immediately obtain a typechecker
%     that is executable. Same applies for the small-step evaluation relation. Translating the relations into functions in Coq is definitely doable but time-consuming, especially since compilation is embedded as part of the typing rules. It is also hard to see whether the function we define really corresponds to the relation unless we formally prove it. This issue is particularly relevant at the early stage of the development when the compilation rules were buggy and the simulation property was violated often as we added new generator cases.
%     In Redex, we don't have any formal guarantee either, but at least we can more easily see the correspondence because Redex is able to convert the relation into an executable version so we can specify the relation literally. This feature of Redex helped us speed up our development significantly when our compilation rules were constantly changing.
% \end{itemize} }
% \item there's a funny change of line spacing in column 2 of page 6, about 2/3 down.
% \end{itemize}
% }

The main subtlety of compiling \checkedc to Clang/LLVM is to capture the annotations on pointer literals
that track array bound information, which is used in premises
of rules like \textsc{S-DefArray} and
  \textsc{S-AssignArr} to prevent spatial safety violations.
The \checkedc compiler \cite{li22checkedc} inserted additional pointer checks 
for verifying pointers are not null and the bounds are within their limits. \lc{what does it mean by bounds within limits?}
The latter is done by introducing additional shadow variables for storing (NT-)array pointer bound information.

\begin{figure}[t!]
{\small
\hspace*{-0.5em}
\begin{tabular}{|c|c|c|c|}
\hline
& \cmode & \tmode & \umode \\
\hline
& \textsc{CBox} / \textsc{Core} & \textsc{CBox} / \textsc{Core} & \textsc{CBox} / \textsc{Core} \\
\hline
\cmode & $\estar{x}$ / $\getstar{\cmode}{x}$ 
 & $\texttt{sand\_get}(x)$ / $\getstar{\umode}{x}$ &  $\times$ \\
\hline
\umode & $\times$
 & $\estar{x}$ / $\getstar{\umode}{x}$ &  $\estar{x}$ / $\getstar{\umode}{x}$ \\
\hline
\end{tabular}

}
\caption{Compiled Targets for Dereference}
\label{fig:flagtable}
\end{figure}

In \systemname, context and pointer modes determine the particular heap/function store that a pointer points to,
i.e., $\cmode$ pointers point to checked regions, while $\tmode$ and $\umode$ pointers point to unchecked regions. \lc{context mode doesn't determine where pointer points to but check if it is valid to point to that region?}
Unchecked regions are associated with a sandbox mechanism that permits exception handling of potential memory failures.
In the compiled LLVM code, pointer access operations have different syntaxes when the modes are different. \lc{should we explain why we use different syntax instead of compiling the corresponding heap to llvm}
\Cref{fig:flagtable} lists the different compiled syntaxes of a deference operation ($\estar{x}$) for the compiler implementation (\textsc{CBox}, stands for \systemname) and formalism (\textsc{Core}, stands for \lang). The columns represent different pointer modes and the rows represent context modes.
For example, when we have a $\tmode$-mode pointer in a $\cmode$-mode region, we compile a deference operation to the sandbox pointer access function ($\texttt{sand\_get}(x)$) accessing the data in the \systemname implementation. In \lang, we create a new deference data-structure on top of the existing $\estar{x}$ operation (in LLVM): $\getstar{m}{x}$. If the mode is $\cmode$, it accesses the checked heap/function store; otherwise, it accesses the unchecked one.

This section shows how \lang deals with pointer modes, mode switching and function pointer compilations, 
with no loss of expressiveness
as the \checkedc contains the erase of annotations in \cite{li22checkedc} and \Cref{appx:comp1}.
For the compiler formalism, 
we present a compilation algorithm that converts from
\lang to \elang, an untyped language without metadata
annotations, which represents an intermediate layer we build on LLVM for simplifying compilation. 
In \elang, the syntax for deference, assignment, malloc, function calls are: $\getstar{m}{e}$, $\elassign{m}{e}{e}$, 
$\emalloc{m}{\omega}$, and $\elcall{m}{e}{\overline{e}}$. \lc{strlen?}
The algorithm sheds
  light on how compilation can be implemented in the real Checked C
  compiler, while eschewing many vital details (\elang has many 
  differences with LLVM IR).


%This section shows how \systemname deals with 
%annotations can be safely erased: using static information a compiler
%can insert code to manage and check bounds metadata, with no loss of
%expressiveness. We present a compilation algorithm that converts from
%\lang to \elang, an untyped language without metadata
%annotations. The syntax and semantics \elang
  %closely mirrors that of \lang; it differs only in that literals lack
  %type annotations and its operational rules perform no
  %bounds and null checks, which are instead inserted during
  %compilation. Our compilation algorithm is evidence that \lang's
  %semantics, despite its apparent use of fat pointers, faithfully
  %represents Checked C's intended behavior. The algorithm also sheds
  %light on how compilation can be implemented in the real Checked C
  %compiler, while eschewing many important details (\elang has many 
  %differences with LLVM IR).

Compilation is defined by extending \lang's
typing judgment as follows:
\[\Gamma;\Theta;\rho \vdash_m e \gg \dot e:\tau\]
There is now a \elang output $\dot e$ and an input $\rho$, which maps
each (NT-)array pointer variable to its mode and
each variable \code{p} to a pair of \emph{shadow
  variables} that keep \code{p}'s up-to-date upper and lower bounds. 
These may differ from the bounds in \code{p}'s type due to bounds
widening.\footnote{Since lower bounds are never widened, the
  lower-bound shadow variable is unnecessary; we include it for uniformity.} 
\lc{These two sentences are confusing. }

% When $\Gamma$,$\Theta$ and $\rho$ are all empty, we write $e \gg \dot e$ rather than the
% complete judgment, implicitly assuming that $e$ is a well-typed and closed
% term.

We formalize rules for this judgment in PLT Redex~\cite{pltredex},
following and extending our Coq development for \lang. To give
confidence that compilation is correct, we use Redex's property-based
random testing support to show that compiled-to $\dot e $ simulates
$e$, for all $e$.

% We developed a \checkedc compiler to compile a \checkedc program to a C program.
% \mwh{We formalized compilation from CoreChkC to a version of CoreChkC
%   but with the metadata removed, right? This is not a Checked C
%   compiler. You go on to see stuff about CompCert, CLight,
%   etc. This is confusing. We should be talking about how this relates
%   to what was just presented. Pick definitive names for things. }
% Given a \checkedc program $e$, we build a compilation process ($\gg$), such that $e \gg \dot e$, where $\dot e$ is the corresponding C program for $e$ in A-normal form (ANF). 
% The compilation process ($\gg$) relies on the type checking step $\Gamma;\Theta\vdash_m e:\tau$. 
% Especially, it relies on $\Gamma$ to provide the type information for variables in $e$. 
% We utilize CompCert/CLight syntax and semantics \cite{Leroy:2009:FVC:1666192.1666216,Blazy2009} as our translation target language.
%  Besides, we defined two data structures in the CompCert format for representing $\enull$ and $\ebounds$ states.
% We write $\xrightarrow{c}$ for the semantics of CLight. \mwh{Of our
%   target language, which I presume does not have all the features that
%   CLight has. Should mention up front that we did all of this in PLT Redex.}

Due to space constraints, we explain the rules for compilation by
examples, using a C-like syntax; the complete rules are given in
\iftr
Appendix~\ref{appx:comp1}.
\else
the supplemental report~\cite{checkedc-tech-report}.
\fi
Each rule performs up to three tasks: (a) conversion of $e$ to
A-normal form; (b) insertion of dynamic checks and bound widening expressions; 
and (c) generate right pointer accessing expressions based on modes.
%
A-normal form conversion is straightforward: compound expressions are managed by storing results of subexpressions into temporary variables,
as in the following example.

{\vspace*{-0.5em}
{\small
\begin{center}
$
\begin{array}{l}
$\code{let y=(x+1)+(6+1)}$
\;
\begin{frame}

\tikz\draw[-Latex,line width=2pt,color=orange] (0,0) -- (1,0);

\end{frame}
\;
\begin{array}{l}
$\code{let a=x+1;}$\\
$\code{let b=6+1;}$\\
$\code{let y=a+b}$\\
\end{array}
\end{array}
$
\end{center}
}
}

This simplifies the management of effects from subexpressions. The
next two steps of compilation are more interesting.
We state them based on different \lang operations.

% \review{Fig 9: if this is actual C code, then your null-check at line 6 will be
%   eliminated by the compiler. At line 3, you performed a pointer addition, which
%   is only defined when `p` is non-null. So, either `p` is non-null, and the
%   NULL-check can be eliminated; or, `p` is NULL, but line 3 was undefined
%   behavior, meaning the compiler is allowed to do anything, notably eliminate
%   the NULL-check. This is where I am super confused, and either:
%   - CoreC is not really the C language, and has different semantics...? but is
%     this well-defined in the context of LLVM?
%   - there is a problem that was not caught by the PLT-Redex-based testing.
% \yiyun{We have clarified at the start of IV that CoreC is an untyped
%   variant of CoreChkC, and does not aim to represent C per se, or LLVM
%   IR. We aimed to avoid confusion by rewriting the examples in a way that is more closely
%     related to the syntax presented in Fig 3. Pointer arithmetic between 0 and a non-zero
%     index is always valid because CoreC there is technically only
%     integer arithmetic.}}

\myparagraph{Pointer Accesses and Modes}
%
In every declaration \lc{(or the beginning of a function body) why?} of a pointer,
if the poniter is an (NT-)array,
we first allocate two \emph{shadow variables}
to track the lower and upper bounds which are potentially changed for pointer arithmetic and NT-array bound widening.
Each $\cmode$-mode NT-array pointer variable is associated with its type information in a store.
Additionally, we place bounds and null-pointer checks, such as the line 6 and 7 in \Cref{fig:compilationexample1}.
In addition, in the formalism, before every use of a tainted pointer (\Cref{fig:compilationexample1} line 9 and 10), 
there is an inserted verification step similar to \Cref{fig:const-type},
which checks if a pointer is well defined in the heap (\code{not_null}) and the spatial safety.
Predicate \code{not_null} checks that every element in the pointer's range (\code{p_lo} and \code{p_hi}) is well defined in the heap.  
The modes in compiled deference (\code{*(mode(p) }$\wedge$\code{ m,p)})
 and assignment (\code{*(mode(q) }$\wedge$\code{ m,q)=1}) operations 
are computed based on the meet 
operation ($\wedge$) of the pointer mode (e.g. \code{mode(p)}) and the current context mode (\code{m}).

\myparagraph{Checked and Unchecked Blocks}
%
In the \systemname implementation,
$\euncheckedtext$ and $\echeckedtext$ blocks 
are compiled as context switching functions provided by the sandbox mechanism.
$\eunchecked{\overline{x}}{e}$ is compiled to 
$\texttt{sandbox\_call}(\overline{x},e)$, where we call the sandbox 
to execute expression $e$ with the arguments $\overline{x}$.
$\echecked{\overline{x}}{e}$ is compiled to 
$\texttt{callback}(\overline{x},e)$, where we perform 
a \texttt{callback} to a checked block code $e$ inside a sandbox.
In \systemname, we adopt an aggressive execution scheme that
directly learns pointer addresses from compiled assembly to make the $\texttt{callback}$ happen.
In the formalism, we rely on the type system to 
guarantee the context switching without creating the extra function calls for simplicity.

%Fig.~\ref{fig:compilationexample} shows how an invocation of
%\code{strlen} on a null-terminated string is compiled into C
%code. Each dereference of a checked pointer requires a null check
%(See \textsc{S-DefNull} in Fig.~\ref{fig:semantics}), which the
%compiler makes explicit: Line~$3$ of the generated code has the null
%check on pointer \code{p} due to the \code{strlen},
%  and a similar check happens
%  at line~$8$ due to the pointer arithmetic on \code{p}.
%Dereferences also require bounds checks: line~$2$ checks \code{p} is
%in bounds before computing \code{strlen(p)}, while line~$10$ does
%likewise before computing \code{*(p+1)}.

\myparagraph{Function Pointers and Calls}
%
Function pointers are managed similarly to normal pointers,
but we insert checks to check if the pointer address is not null in 
the function store instead of heap, and whether or not the type is correctly represented, 
for both $\cmode$ and $\tmode$ mode pointers 
\footnote{$\cmode$-mode pointers are checked once in the beginning and $\tmode$-mode pointers are checked every time when use}.
For example, in compiling the \code{stringsort} function in \Cref{fig:checkedc-example-1},
we place a check \code{verify_fun(cmp, not_null(c, p_lo, p_hi) && type_match)},
and we place a similar check before \Cref{fig:checkedc-example-3} line 7 to check the tainted \code{cmp} when it is used. 
The compilation of function calls (compiling to $\elcall{m}{e}{\overline{e}}$) 
is similar to the manipulation of pointer access operations in \Cref{fig:flagtable}.

For compiling dependent function calls,
\Cref{fig:compilationexample1} provides a hint.
Notice that the bounds for the array pointer \code{p} are not passed as
arguments. Instead, they are initialized according to \code{p}'s
type---see line~4 of the original \lang program at the top of the figure.
Line~$3$ of the generated code
sets the lower bound  to \code{0} and the
upper bound to \code{n}.

\subsection{Metatheory}
\label{sec:meta}

% \review{ 
% My interpretation of section IV.C is that the authors have a pen-and-paper
%   proof of Theorem 4, and that random testing using the Redex models has been
%   used to gain confidence in such a proof. Is this correct?}
% \liyi{Yes, this is correct.}
% \mwh{No, not correct: We have no pen-and-paper proof, just the
%   model. Clarify here}
% showing that
% extensive random testing fails to falsify the bisimilarity
% property. \mwh{Better way to state the previous?}

We formalize both the compilation procedure and the simulation
theorem in the PLT Redex model we developed for \lang (see Sec.~\ref{sec:syntax}),
and then attempt to falsify it via Redex's support for random
testing. Redex allows us
  to specify compilation as logical rules (an extension
  of typing), but then execute it algorithmically to
  automatically test whether simulation holds. This process revealed
  several bugs in compilation and the theorem statement.
%
  % us gain confidence that our original pen and paper proof of
  % simulation remains true with the addition of variable bounds. }
We ultimately plan to prove simulation in the Coq model.

%Turning to the simulation theorem: We first introduce notation
%used to specify the theorem.
We use the notation $\gg$ to
indicate the \emph{erasure} of stack and heap---the rhs is the same as
the lhs but with type annotations removed:
\begin{equation*}
  \begin{split}
    \heap  \gg & \dot \heap \\
    \varphi \gg & \dot \varphi
  \end{split}
\end{equation*}
In addition, when $\Gamma;\emptyset\vdash
\varphi$ and $\varphi$ is well-formed, we write $(\varphi,\heap,e) \gg_m (\dot \varphi, \dot \heap,
\dot e)$ to denote $\varphi \gg \dot \varphi$, $\heap \gg \dot \heap$
and $\Gamma;\Theta;\emptyset \vdash_m e \gg \dot e : \tau$ for some $\tau$ respectively. $\Gamma$ is omitted from the notation since the well-formedness of $\varphi$ and its consistency with respect to $\Gamma$ imply that $e$ must be closed under $\varphi$, allowing us to recover $\Gamma$ from $\varphi$.
Finally, we use $\xrightarrow{\cdot}^*$ to denote the transitive closure of the
reduction relation of $\elang$. Unlike the $\lang$, the semantics of
$\elang$ does not distinguish checked and unchecked regions.

Fig.~\ref{fig:checkedc-simulation-ref} gives an overview of 
the simulation theorem.\footnote{We ellide the  possibility of $\dot e_1$ evaluating to $\ebounds$ or $\enull$ in the diagram for readability.} The simulation theorem is specified in a way
that is similar to the one by~\citet{merigoux2021catala}.

An ordinary simulation property would
replace the middle and bottom parts of the figure with the
following: \[(\dot \varphi_0, \dot \heap_0, \dot e_0) 
  \xrightarrow{\cdot}^* (\dot \varphi_1, \dot \heap_1, \dot e_1)\]
Instead, we relate two erased configurations using the relation $\sim$,
which only requires that the two configurations will eventually reduce
to the same state.

\ignore{
 We formulate our simulation theorem differently
because the standard simulation theorem imposes a very strong
syntactic restriction to the compilation strategy. Very often, $(\dot
\varphi_0, \dot \heap_0, \dot e_0)$ reduces to a term that is
semantically equivalent to $(\dot \varphi_1, \dot \heap_1, \dot e_1)$,
but we are unable to syntactically equate the two configurations due
to the extra binders generated for dynamic checks and ANF
transformation. In earlier versions of the Redex model, we attempted
to change the compilation rules so the configurations could match
syntactically. However, the approach scaled poorly as we added
additional rules. 
This slight relaxation on the equivalence relation
between target configurations allows us to specify compilation more
naturally without having to worry about syntactic constraints.
}

% The two theorems are translation preservation and simulation. We donate $\xrightarrow{c}$ as the transition semantics of CLight.
\begin{thm}[Simulation ($\sim$)]\label{simulation-thm}
For \lang expressions $e_0$, stacks $\varphi_0$, $\varphi_1$, and heap snapshots $\heap_0$, $\heap_1$, 
if $\heap_0 \vdash \varphi_0$, $(\varphi_0,\heap_0,e_0)\gg_c (\dot \varphi_0,\dot \heap_0, \dot e_0)$,
and if there exists some $r_1$ such that $(\varphi_0, \heap_0, e_0)
\rightarrow_c (\varphi_1, \heap_1, r_1)$, then the following facts hold:

\begin{itemize}

\item if there exists $e_1$ such that $r=e_1$ and $(\varphi_1, \heap_1, e_1) \gg (\dot \varphi_1, \dot \heap_1, \dot e_1)$, then there exists some $\dot \varphi$,$\dot \heap$, $\dot e$, such that
$(\dot \varphi_0, \dot \heap_0,\dot e_0) \xrightarrow{\cdot}^* (\dot
\varphi,\dot \heap,\dot e)$ and $(\dot
\varphi_1,\dot \heap_1,\dot e_1) \xrightarrow{\cdot}^* (\dot \varphi,
\dot \heap,\dot e)$.

\item if $r_1 = \ebounds$ or $\enull$, then we have $(\dot \varphi_0, \dot \heap_0,\dot e_0) \xrightarrow{\cdot}^* (\dot
\dot \varphi_1,\dot \heap_1, r_1)$ where $\varphi_1 \gg \dot
\varphi_1$, $\heap_1 \gg \dot \heap_1$.

\end{itemize}
\end{thm}


% when $r_1 = e_1$ for
% some $e_1$ and
% $(\varphi_1, \heap_1, e_1) \gg (\dot \varphi_1, \dot \heap_1, \dot e_1)$, then
% there exists some $\dot \varphi$,$\dot \heap$, $\dot e$, such that
% $(\dot \varphi_0, \dot \heap_0,\dot e_0) \xrightarrow{\cdot}^* (\dot
% \varphi,\dot \heap,\dot e)$ and $(\dot
% \varphi_1,\dot \heap_1,\dot e_1) \xrightarrow{\cdot}^* (\dot \varphi,
% \dot \heap,\dot e)$. When $r_1 = \ebounds$ or $\enull$, we have $(\dot \varphi_0, \dot \heap_0,\dot e_0) \xrightarrow{\cdot}^* (\dot
% \dot \varphi_1,\dot \heap_1, r_1)$ where $\varphi_1 \gg \dot
% \varphi_1$, $\heap_1 \gg \dot \heap_1$.

% \begin{thm}[Simulation ($\sim$)]\label{simulation-thm}
% For \lang expressions $e_0$, stacks $\varphi_0$, $\varphi_1$, and heap snapshots $\heap_0$, $\heap_1$, 
% if $\emptyset;\emptyset;\emptyset \vdash_\cmode e_0 \gg \dot e_0 :\tau_0$,
% and if there exists some $r_1$ such that $(\varphi_0, \heap_0, e_0)
% \rightarrow_\cmode (\varphi_1, \heap_1, r_1)$, when $r_1 = e_1$ for
% some $e_1$ and
% $\emptyset;\emptyset;\emptyset \vdash_\cmode e_1 \gg \dot e_1 :\tau_1$ where $\tau_1 \sqsubseteq \tau_0$
% , then
% there exists some $\dot \varphi$,$\dot \heap$, $\dot e$, such that
% $(\dot \varphi_0, \dot \heap_0,\dot e_0) \xrightarrow{\cdot}^* (\dot
% \varphi,\dot \heap,\dot e)$ and $(\dot
% \varphi_1,\dot \heap_1,\dot e_1) \xrightarrow{\cdot}^* (\dot \varphi,
% \dot \heap,\dot e)$. When $r_1 = \ebounds$ or $\enull$, we have $(\dot \varphi_0, \dot \heap_0,\dot e_0) \xrightarrow{\cdot}^* (\dot
% \dot \varphi_1,\dot \heap_1, r_1)$ where $\varphi_1 \gg \dot
% \varphi_1$, $\heap_1 \gg \dot \heap_1$.
% \end{thm}

As our random generator never generates
$\euncheckedtext$ expressions (whose behavior could be undefined), we can only test a the simulation theorem 
as it relates to checked code. This limitation makes it
unnecessary to state the other direction of the simulation theorem
where $e_0$ is stuck, because Theorem~\ref{thm:progress} guarantees
that $e_0$ will never enter a stuck state if it is well-typed in
checked mode.

The current version of the Redex model has been tested against $23000$
expressions with depth less than $11$. Each expression can
reduce multiple steps, and we test simulation between every two
adjacent steps to cover a wider range of programs, particularly the
ones that have a non-empty heap.

\begin{figure}[t]
{\small
\[
\begin{array}{c}
\begin{tikzpicture}[
            > = stealth, % arrow head style
            shorten > = 1pt, % don't touch arrow head to node
            auto,
            node distance = 3cm
        ]

\begin{scope}[every node/.style={draw}]
    \node (A) at (0,1.5) {$\varphi_0,\heap_0, e_0$};
    \node (B) at (4,1.5) {$\varphi_1, \heap_1 ,e_1$};
    \node (C) at (0,0) {$\dot \varphi_0, \dot \heap_0 ,\dot e_0$};
    \node (D) at (4,0) {$\dot \varphi_1, \dot \heap_1, \dot e_1$};
    \node (E) at (2,-1.5) {$\dot \varphi,\dot \heap ,\dot e$};
\end{scope}
\begin{scope}[every edge/.style={draw=black}]

    \path [->] (A) edge node {$\longrightarrow_\cmode$} (B);
    \path [<->] (A) edge node {$\gg$} (C);
    \path [<->] (B) edge node {$\gg$} (D);
    \path [dashed,<->] (C) edge node {$\sim$} (D);
    \path [dashed,->] (C) edge node {$\xrightarrow{\cdot}^*$} (E);
    \path [dashed,->] (D) edge node[above] {$\xrightarrow{\cdot}^*$} (E);
\end{scope}

\end{tikzpicture}
\end{array}
\]
}
\caption{Simulation between \lang and \elang }
\label{fig:checkedc-simulation-ref}
\end{figure}

% \ignore
% {The two theorems can be best understood by the diagram in Fig.~\ref{fig:checkedc-simulation-ref}.
% % The diagram also indicates the simulation relation ($\sim$) among our formalization of the \checkedc Type System ($\vdash$), Semantics ($\longrightarrow$), Compilation ($\gg$), as well as the compiled C semantics ($\xrightarrow{c}$).
% On the top of the relation, we have type soundness theorem (Sec.~\ref{sec:theorem}) stating that every type-checked \lang program $e_0$ can transition via \lang semantics into a type-checked program $e_1$ by taking a checked step.
% The second line refers to that both $e_0$ and $e_1$ can be compiled to a C program $\dot e_0$ and $\dot e_1$, and such compilations always exist. The bottom two rewrites ($\xrightarrow{c}$) state that both $\dot e_0$ and $\dot e_1$ are transitioned finally to the same state $\dot e$. 

%  The simulation states that given any well-typed \lang expression $e_0$, its compiled C program as $\dot e_0$, if $e_0$ can transition one step to $e_1$, and there must exist the compiled C program $\dot e_1$, such that both $\dot e_0$ and $\dot e_1$ transition to the same final state $\dot e$ in C. The simulation relation $\sim$ is built on top of the "triangle" structure in Fig.~\ref{fig:checkedc-simulation-ref} stating that every \lang expression and its one step transitioned expression might be compiled to two different C program, but the C semantics always evaluates them to the same place.
% The translation preservation theorem is a support for the simulation theorem stating that for any well-typed \lang expression $e_0$,  its one step \checkedc transition expression $e_1$ must exist a compiled C program through the compilation process ($\gg$).

% A corollary of the simulation theorem states that for any \lang expression $e_0$ and its one step \lang transition expression $e_1$, such that $e_0 \longrightarrow e_1$, if $e_0$ contains a fault due to a unchecked block (the blame theorem tells us that if there is a fault in a \lang code, it must come from a unchecked block), and its compiled code $\dot e_0$ is evaluated to a fault state, the compiled code $\dot e_1$ for $e_1$ is always evaluated to the same fault state in C.
% This is an important property for a \lang compiler to preserve because any problematic program will be captured by running it long enough, so that enough random testing cases are able to capture all the bugs.
% In our formalization implementation, the type soundness theorem is proved through Coq, while the simulation theorem is validated through intensive random test case generation. We generate tens of thousands cases and cover all possible corner cases of the compilation. 
% }
% \ignore{
% \begin{figure*}[t]
%   \begin{subfigure}[b]{1\textwidth}
%     \begin{lstlisting}
% int deref_array (size_t n, nt_array_ptr<int> p : bounds(p, p + n)) {
%   return *p;
% }

% ...
% /* nt_array_ptr<int> p0 : bounds(p0, p0 + 5) */
% deref_array(5, p0);
%     \end{lstlisting}
%     \label{fig:chkcexamplederef}
%     \caption{\texttt{deref\_array} in \checkedc}
%   \end{subfigure}

%   \begin{subfigure}[b]{1\textwidth}
% \begin{lstlisting}
% int deref_array(size_t n, int *p) {
%   /* statically determine the definitions p_lo and p_hi */
%   int *p_lo = p;
%   int *p_hi = p + n;
%   /* runtime checks */
%   assert(p_lo <= p && p <= p_hi);
%   assert(p != NULL);
%   /* widening */
%   int p_derefed = *p;
%   if (p_derefed != '\0') {
%     if (p_hi == p) {
%       ++p_hi;
%     }
%   }
%   return p_derefed;
% }
% ...
% /* int *p0 */
% /* rho(p0) = p_lo, p_hi */
% deref_array(5, p0);
% \end{lstlisting}
% \caption{\texttt{deref\_array} in C}
%     \label{fig:cexamplederef}
%   \end{subfigure}

%   \begin{subfigure}[b]{\textwidth}
%     \begin{lstlisting}
% /* nt_array_ptr<int> p : bounds(e0, e1) */
% /* rho(p) = p_lo, p_hi */
% size_t x = strlen(p);
% if(x >= 1) {
%   *(p+1);
% }
%     \end{lstlisting}
%     \caption{\texttt{strlen} in \checkedc}
%     \label{fig:chkcexamplestrlen}
%   \end{subfigure}

%   \begin{subfigure}[b]{\textwidth}
%     \begin{lstlisting}
% /* nt_array_ptr<int> p : bounds(e0, e1) */
% /* rho(p) = p_lo, p_hi */
% /* runtime checks omitted */
% ...
% size_t x = strlen(p);
% int *p_hi_new = p + x;
% p_hi = max(p_hi, p_hi_new);
% if (x >= 1) {
%   /* null check for pointer arithmetic */
%   assert(p != NULL);
%   int *p_1 = p + 1;
%   /* null check for dereference */
%   assert(p_1 != NULL);
%   /* bounds check for dereference */
%   /* note how we are using p_hi rather than p + x */
%   assert(p_lo <= p_1 && p_1 <= p_hi);
%   ...
% }
%     \end{lstlisting}
%     \caption{\texttt{strlen} in C}
%     \label{fig:cexamplestrlen}
%   \end{subfigure}
% \caption{Compilation of null-terminated string dereference}
% \label{fig:compilationexample}
% \end{figure*}
% }

% \ignore{
% Fig.~\ref{fig:compilationexample} gives an example of how we compile the
% dereference of null-terminated strings by inserting explicit checks and
% bounds widening code. For readability, we present the example in real
% \checkedc and C syntax. At
% line 3-4 of Fig.~\ref{fig:cexamplederef}, we see how the upper and lower
% bounds are defined in terms of the arguments \code{p} and \code{n}
% according to the bounds annotations obtained during typechecking. This
% avoids the need to pass in bounds as extra arguments, thus maintaining
% compatibility with C code. When we call \code{deref_array} on
% \code{p0}, a string with size 5 (excluding the null character), there
% is no need to pass the bounds \code{p_lo} and \code{p_hi} stored on
% the stack. It is possible that \code{p_hi} has been widened
% before we reach line 21. The programmer will have to perform a
% \code{dynamic_bounds_cast} to recover the more precise bounds information. The if statement from line 10 to line 14
% attempts to widen the bound when the dereferenced result is not
% null. \todo[inline]{Show a full-fledged C program?} The widened upper bound will be available within the scope of
% \code{deref_array}, in contrast to the T-IfNT rule, which only
% remembers the widened bounds within the scope of the then branch of
% the if statement. 


% \todo[inline]{Widening can happen at every
%   dereference at runtime. Is that ok?} Fig.~\ref{fig:cexamplestrlen}
% shows how an invocation of \code{strlen} on null-termianted strings
% is compiled into C code. We perform the same runtime checks that
% happen during dereferencing. The widening code at line
% 8 updates \code{p}'s upper bound only if the result of
% \code{strlen} is larger than the value of the upper bound stored on
% the stack. This is another scenario where the runtime can be more precise
% than the statically determined bounds information.



% }

\subsection{Additional Type and Semantic Rules}\label{appx:add-type-sem}

\begin{DIFnomarkup}
\begin{figure}
{\small
\[\hspace*{-1.2em}
\begin{array}{l}
\textcolor{blue}{\text{Bound Inequality and Equality:}}\\[0.3em]
  \begin{array}{r@{~}c@{~}l@{~}c@{~}l}
     n \le n' &\Rightarrow& n &\le_{\Theta} & n'\\
     n \le n' &\Rightarrow& x+n &\le_{\Theta} & x+n'\\
     n \le n' \wedge \Theta(x)=\tgez &\Rightarrow& n &\le_{\Theta} & x+n'\\
     \Theta(x)=\teq{b} \wedge b+n\le_{\Theta}b'  &\Rightarrow& x+n & \le_{\Theta} & b'\\
     \Theta(x)=\teq{b}\wedge b'\le_{\Theta}b+n  &\Rightarrow& b' & \le_{\Theta} & x+n\\
     b \le_{\Theta} b' \wedge b' \le_{\Theta} b  &\Rightarrow& b & =_{\Theta} & b'
    \end{array}
  \\[1.5em]
\textcolor{blue}{\text{Type Equility:}}\\
  \begin{array}{r@{~}c@{~}l@{~}c@{~}l}
     && \tint & =_{\Theta} & \tint\\
     \omega =_{\Theta} \omega' &\Rightarrow& \tptr{\omega}{\xi} & =_{\Theta} & \tptr{\omega'}{\xi}\\
     \bvar =_{\Theta} \bvar' \wedge  \tau =_{\Theta} \tau'
             &\Rightarrow& \tallarrayb{\bvar}{\tau} & =_{\Theta} & \tallarrayb{\bvar'}{\tau'}\\

    \textit{cond}(\overline{x},\overline{\tau}\to\tau,\overline{y},\overline{\tau'}\to\tau')

 &\Rightarrow& \tfun{\overline{x}}{\overline{\tau}}{\tau} & 
                         =_{\Theta} & \tfun{\overline{y}}{\overline{\tau'}}{\tau'}\\
    \end{array}
  \\[0.7em]
\textcolor{blue}{\text{Subtype:}}\\[0.3em]

  \begin{array}{r@{~}c@{~}l@{~}c@{~}l}
    \tau =_{\Theta} \tau'&\Rightarrow&\tau &\sqsubseteq_{\Theta}& \tau'\\[0.2em]

    0\le_{\Theta} b_l \wedge b_h \le_{\Theta} 1 &\Rightarrow& \tptr{\tau}{m}&\sqsubseteq_{\Theta}& \tarrayptr{b_l}{b_h}{\tau}{m}\\[0.2em]
    b_l \le_{\Theta} 0 \wedge 1 \le_{\Theta} b_h &\Rightarrow& \tarrayptr{b_l}{b_h}{\tau}{m} &\sqsubseteq_{\Theta}& \tptr{\tau}{m}\\[0.2em]
    b_l \le_{\Theta} 0 \wedge 1 \le_{\Theta} b_h &\Rightarrow& \tntarrayptr{b_l}{b_h}{\tau}{m} &\sqsubseteq_{\Theta}& \tptr{\tau}{m}\\[0.2em]
    %% b_l \le b_l' \wedge b_h' \le b_h &\Rightarrow&  \tarrayptr{b_l}{b_h}{\tau}{m} &\sqsubseteq&  \tarrayptr{b_l'}{b_h'}{\tau}{m}\\[0.6em]
    b_l \le_{\Theta} b_l' \wedge b_h' \le_{\Theta} b_h &\Rightarrow& \tntarrayptr{b_l}{b_h}{\tau}{m} &\sqsubseteq_{\Theta}& \tarrayptr{b_l'}{b_h'}{\tau}{m}\\[0.6em]
    b_l \le_{\Theta} b_l' \wedge b_h' \le_{\Theta} b_h &\Rightarrow& \tallarrayptr{b_l}{b_h}{\tau}{m} &\sqsubseteq_{\Theta}& \tallarrayptr{b_l'}{b_h'}{\tau}{m}
\\[0.2em]
\overline{\tau'}\sqsubseteq_{\Theta}\overline{\tau}\wedge \tau\sqsubseteq_{\Theta}\tau' &\Rightarrow& \tptr{\tfun{\overline{x}}{\overline{\tau}}{\tau}}{\xi} &\sqsubseteq_{\Theta}& \tptr{\tfun{\overline{x}}{\overline{\tau'}}{\tau'}}{\xi}

    \end{array}
\end{array}
  \]
}
{\footnotesize
\[
\begin{array}{l}
n'+n = add(n',n)
\qquad
(x+n')+n = x+add(n',n)\\
\textit{cond}(\overline{x},\tau,\overline{y},\tau')
=\exists\overline{z}\;.\;\overline{x}\cupdot\overline{z}
  \wedge \overline{y}\cupdot\overline{z}
  \wedge \size(\overline{x})=\size(\overline{y})=\size(\overline{z})
\\\qquad\qquad\qquad\qquad\qquad
  \wedge \tau[\overline{z}/\overline{x}]= \tau'[\overline{z}/\overline{x}]
\end{array}
\]
}
  \caption{Type Equality and Subtyping}
  \label{fig:checkc-subtype}
\end{figure}
\end{DIFnomarkup}

Since bounds expressions may
contain variables, determining assumptions like $b_l \leq b_l'$
requires reasoning about the probable values of these variables'. The type
system uses $\Theta$ to make such reasoning more precise.
$\Theta$ is a map from variables $x$ to
equation predicates $P$, which have the form $P ::= \tgez \;|\; \teq{b}$.
It maps variables to equations that are recorded along the type checking procedure.
If $\Theta$ maps $x$ to $\tgez$, that means that $x \ge 0$;
$\teq{b}$ means that $x$ is equivalent to the bound value $b$ in the current context, 
such as in the type judgment for $e_2$ in Rule \textsc{T-LetInt} and \textsc{T-RetInt}.
\iftr
  Appendix~\ref{app:le}.
\else
  the supplemental report~\cite{checkedc-tech-report}.
\fi has an example rule for populating $\Theta$ with a $\tgez$ predicate.

\begin{figure}[t]
{\small
{\captionsetup[lstlisting]{margin = 8 mm}
  \begin{lstlisting}[xleftmargin=8 mm]
nt_array_ptr<char> safe_strcat
   (nt_array_ptr<char> dst : count(n),
    nt_array_ptr<char> src : count(0), int n) {
  int x = strlen(dst);
  int y = strlen(src);
  nt_array_ptr<char> c : count(n) =
    dyn_bounds_cast
           <nt_array_ptr<char>>(dst,count(n));
    // sets c == dst with bound n (not x)
  if (x+y < n) {
    for (int i = 0; i < y; ++i)
      *(c+x+i) = *(src+i);
    *(c+x+y) = '\0';
    return dst;
  }
  return null;
}
  \end{lstlisting}
}
}
\caption{Implementation of safe \code{strcat}}
\label{fig:strcat-ex}
\end{figure}

Recall that array
bounds in types may refer to in-scope variables; e.g., parameter
\code{dst}'s bound \code{count(n)} refers to parameter \code{n} on lines
2-3 in \Cref{fig:strcat-ex}. 
All dependent bound variables appearing in function headers 
are bounded by a variable list in the function pointer type, which is checked in the type rule.
Semantically,
the call is expanded into a \texttt{let} which binds
parameter variables $\overline{x}$ to the actual arguments
$\overline{n}$, but annotated with the parameter types
$\overline{\tau}$ (this will be safe for type-correct programs). 
The function body $e$ is wrapped in a static cast
$(\tau[\overline{n} / \overline{x}])$ which is the function's return
type but with any integer parameter variables $\overline{x}$ appearing in that
type, as type bound variables,
substituted with the call's actual arguments $\overline{n}$.
To see why this is needed, suppose that \code{safe_strcat} in
Fig.~\ref{fig:strcat-ex} is defined to return a
\code{nt_array_ptr<int>:count(n)} typed term, and assume that we
perform a \code{safe_strcat} function call as
\code{x=safe_strcat(a,b,10)}. After the evaluation of \code{safe_strcat}, the
function returns a value with type \code{nt_array_ptr<int>:count(10)}
because we substitute bound variable \code{n} in the 
defined return type with \code{10} from the function call's
argument list.


\begin{figure}[t!]
  \begin{small}
\begin{lstlisting}[mathescape,xleftmargin=4 mm]
int deref_array(n : int,
     p :  $\color{green!40!black}\tntarrayptr{0}{n}{\tint}{\cmode}$,
     q : $\color{green!40!black}\tntarrayptr{0}{n}{\tint}{\tmode}$) {
  /* $\color{purple!40!black}\rho$(p) = p_lo,p_hi,p_m */
  /* $\color{purple!40!black}\rho$(q) = q_lo,q_hi,q_m */
    * p;
    * q = 1;
}
...
/* p0 : $\color{purple!40!black}\tntarrayptr{0}{5}{\tint}{\cmode}$ */
/* q0 : $\color{purple!40!black}\tntarrayptr{0}{5}{\tint}{\tmode}$ */
deref_array(5, p0, q0);
    \end{lstlisting}
\begin{frame}

\tikz\draw[-Latex,line width=2pt,color=orange] (0,0) -- (1,0);

\end{frame}
\begin{lstlisting}[mathescape,xleftmargin=4 mm]
deref_array(int n, int* p, int * q) {
  //m is the current context mode
  let p_lo = 0; let p_hi = n; 
  let q_lo = 0; let q_hi = n; 
  /* runtime checks */
  assert(p_lo <= 0 && 0 <= p_hi);
  assert(p != 0);
  *(mode(p) $\wedge$ m,p);
  verify(q, not_null(m, q_lo, q_hi) 
             && q_lo <= 0 && 0 <= q_hi);
  *(mode(q) $\wedge$ m,q)=1;
}
...
deref_array(5, p0, q0);
    \end{lstlisting}
\end{small}
    \caption{Compilation Example for Dependent Functions}
\label{fig:compilationexample1}
\end{figure}

For compiling dependent function calls,
\Cref{fig:compilationexample1} provides a hint.
Notice that the bounds for the array pointer \code{p} are not passed as
arguments. Instead, they are initialized according to \code{p}'s
type---see line~4 of the original \lang program at the top of the figure.
Line~$3$ of the generated code
sets the lower bound  to \code{0} and the
upper bound to \code{n}.



\begin{defi}[Type Environment Well-formedness]\label{type-wellformed}
A type environment $\Gamma$ is well-formed if every variable mentioned as type bounds in $\Gamma$ are bounded by $\tint$ typed variables in $\Gamma$.
\end{defi}

\begin{defi}[Heap Well-formedness]
For every $m$, A heap $\heap$ is well-formed if (i) $\heap(m,0)$ is undefined, and
(ii) for all $\evalue{n}{\tau}$ in the range of $\heap(m)$, type $\tau$
contains no free variables. 
\end{defi}

\begin{defi}[Stack Well-formedness]
A stack snapshot $\varphi$ is well-formed if
for all $\evalue{n}{\tau}$ in the range of $\varphi$, type $\tau$
contains no free variables. 
\end{defi}

We also need to introduce a notion of
\emph{consistency}, relating heap environments before and after a
reduction step, and type environments, predicate sets, and stack
snapshots together.


\begin{defi}[Stack Consistency]
A type environment $\Gamma$, variable predicate set $\Theta$, and
stack snapshot $\varphi$ are consistent---written $\Gamma;\Theta\vdash
\varphi$---if for every variable $x$, $\Theta(x)$ is defined implies
$\Gamma(x) = \tau$ for some $\tau$ and 
$\varphi(x) =\evalue{n}{\tau'}$ for some $n,\tau'$ where $\tau' \sqsubseteq_{\Theta} \tau$. 
\end{defi}

\begin{defi}[Checked Stack-Heap Consistency]
A stack snapshot $\varphi$ is consistent with heap $\heap$---written $\heap \vdash \varphi$---if
for every variable $x$, $\varphi(x)= \evalue{n}{\tau}$ with $\mode(\tau)=\cmode$ implies $\emptyset;\heap(\cmode);\emptyset \vdash_{\cmode} n:\tau$.
\end{defi}

\begin{defi}[Checked Heap-Heap Consistency]
A heap $\heap'$ is consistent with $\heap$---written $\heap \triangleright \heap'$---if
for every constant $n$, $\emptyset;\heap;\emptyset \vdash_{\cmode} n:\tau$ implies $\emptyset;\heap';\emptyset \vdash_{\cmode} n:\tau$.
\end{defi}

\subsection{Additional Program evaluations}\label{appx:add-prog-eval}

\subsubsection{parsons}
Parsons is annotated comprehensively in two variants parsons\_wasm and parsons\_tainted. parsons\_wasm has most of its input parsing functions moved into the sandbox, whilst having all its pointers marked as tainted. These sandboxed functions interact with the checked region by making indirect calls through RLBOX's callback mechanism. However, with parsons\_tainted, we do not move any of the functions to the sandbox but still mark all the pointers as tainted. The test suite itself consists of 328 tests comprehensively testing the JSON parser's functionality. Benchmarks for both of these forks are recorded using the mean difference between the \systemname and generic-C/checked-C variants when executing 10 consecutive iterations of the test suite. parsons\_wasm expectedly shows 200/266\% runtime overhead when evaluated against checked-c and generic-c respectively due to the performance limitation of WebAssembly. However, evaluating parsons\_tainted against checked-c shows \systemname to be faster because \systemname by itself performs lighter run-time-instrumentation on tainted pointers as compared to the run-time bounds checking performed on checked pointers by checked-c. Furthermore, we only see an average peak memory of 9.5 KiB as compared to the anticipated 82 KiB overhead as Valgrind does not consider the WASM Shadow memory allocated to the tainted pointers.

\subsubsection{LibPNG}
\systemname changes for libPNG is narrow in scope and begins with the encapsulation CVE-2018-144550 and a buffer overflow in compare\_read(). However, we also annotate sections of Lib-png that involve reading, writing, and image processing (interlace, intrapixel, etc) on user-input image data as tainted. That is, rows of image bytes are read into tainted pointers and the taintedness for the row\_bytes is propagated throughout the program. All our changes extend to the png2pnm and pnm2png executables. To evaluate png2pnm, we take the mean of 10 iterations of a test script that runs png2pnm on 52 png files located within the libpng's pngsuite. To test pnm2png, we take the mean of 10 iterations of pnm2png in converting a 52MB 5184x3456 pixels large pnm image file to png. Valgrind's reported lower Heap space consumption for \systemname converted code is due to the discounted space consumed on the heap by the Sandbox's shadow memory. Consequently, when evaluating pnm2png, \systemname's heap consumption was 52 MB lower as the entire image was loaded onto the shadow memory.  

\subsubsection{MicroHTTPD}
MicroHTTPD demonstrates the practical difficulties in converting a program to \systemname. Our conversion for this program was aimed at sandboxing memory vulnerabilities CVE-2021-3466 and CVE-2013-7039. CVE-2021-3466 is described as a vulnerability from a buffer overflow that occurs in an unguarded "memcpy" which copies data into a structure pointer (struct MHD\_PostProcessor pp) which is type-casted to a char buffer (char *kbuf = (char *) \&pp[1]). Our changes would require making the "memcpy" safe by marking this pointer as tainted. However, this would either require marshaling the data pointed by this structure (and its sub-structure pointer members) pointer or would require marking every reference to this structure pointer as tainted, which in turn requires every pointer member of this structure to be tainted. Marshalling data between structure pointers is not easy and demands substantial marshaling code due to the spatial non-linearity of its pointer members unlike a char*. This did not align with our conversion goals which were aimed at making minimal changes. Consequently, the above CVE stands un-handled by \systemname.  Our changes for CVE-2013-7039 involve marking the user input data arguments of this function as tainted pointers and in the interests of seeking minimal conversion changes, we do not propagate the tainted-ness on these functions. Following up on the chronological impossibility of sandboxing bugs before they are discovered and the general programmer intuiting, we moved many of the core internal functions (like MHD\_str\_pct\_decode\_strict\_() and MHD\_http\_unescape()) into the sandbox. 

\subsubsection{Tiny-bignum}
Due to its small size and simplicity, \systemname changes for Tiny-bignum was chosen to be comprehensive. Furthermore, bignum\_to\_string() was moved to the sandbox due to a memory-unsafe use of sprintf(). Given that significant \systemname conversion efforts is attributed to understanding the source-code and finding the precise extent to which we choose to propagate the taintedness or to stop and give up to marshalling the data between regions, Tiny-bignum only required 4 hours. The evaluation was performed on Tiny-bignum's test suite consisting of 4 Test cases, each of which test the functionality to scale on big numbers subject to all of the supported unary and binary operations.  